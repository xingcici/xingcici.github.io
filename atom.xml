<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一方天地</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://xingcici.github.io/"/>
  <updated>2021-02-02T03:03:09.874Z</updated>
  <id>http://xingcici.github.io/</id>
  
  <author>
    <name>行词</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HTTPS 加密协商过程</title>
    <link href="http://xingcici.github.io/2021/02/01/https-encryption-negotiation-process.html"/>
    <id>http://xingcici.github.io/2021/02/01/https-encryption-negotiation-process.html</id>
    <published>2021-02-01T09:03:59.926Z</published>
    <updated>2021-02-02T03:03:09.874Z</updated>
    
    <content type="html"><![CDATA[<p>HTTPS协商过程中涉及两个角色：client、server。client 大部分情况下可以认为是浏览器。这里会省略去一些加密协议之类的东西。</p><p>1、client 生成随机数 client_random，发送给 server。</p><p>2、server 生成随机数 server_random，再加上证书和签名，返回给 client，证书中包含公钥。</p><p>3、client 对证书进行 hash 生成 hash 串，然后用浏览器内置的 CA 公钥对签名进行验证，对比 hash 串和签名验证结果。</p><p>4、client 再生成一个随机数，与 client_random和 server_random 一起生成一个 shared secret(最终加密用),  再对这次生成的随机数用公钥进行加密生成（pre-master key），发送给 server。同时发送一个用 shared secret 加密的验证消息。</p><p>5、server 对 pre-master key 用私钥进行解密，获得随机数，再用 client_random、server_random 一起也算出 shared secret，对 client 发送的验证消息进行验证。再回复 client 一个也用 shared secret 加密的数据，表示自己准备好了。</p><p>6、在后续的传输过程中，还会对传输内容进行摘要，来验证内容的完整性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;HTTPS协商过程中涉及两个角色：client、server。client 大部分情况下可以认为是浏览器。这里会省略去一些加密协议之类的东西。&lt;/p&gt;
&lt;p&gt;1、client 生成随机数 client_random，发送给 server。&lt;/p&gt;
&lt;p&gt;2、server 生
      
    
    </summary>
    
    
      <category term="加密" scheme="http://xingcici.github.io/categories/%E5%8A%A0%E5%AF%86/"/>
    
    
      <category term="https" scheme="http://xingcici.github.io/tags/https/"/>
    
  </entry>
  
  <entry>
    <title>synchronized 和 static synchronized</title>
    <link href="http://xingcici.github.io/2021/01/28/synchronized-and-static.html"/>
    <id>http://xingcici.github.io/2021/01/28/synchronized-and-static.html</id>
    <published>2021-01-28T10:03:14.200Z</published>
    <updated>2021-01-28T10:08:00.494Z</updated>
    
    <content type="html"><![CDATA[<p>挺简单的一个问题，今天被人问起了顺手写一下。</p><p>其实 synchronized 是实例对象锁，而 static 是类对象锁。每个类类对象在一个classloader只会有一个。</p><p>下面两段简单的代码就能验证</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">package lock;</span><br><span class="line"></span><br><span class="line">import com.meta.mq.common.utils.ThreadUtil;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @author : haifeng.pang.</span><br><span class="line"> * @version 0.1 : Human v0.1 2021&#x2F;1&#x2F;28 下午5:48 By haifeng.pang.</span><br><span class="line"> * @description :</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class Human &#123;</span><br><span class="line"></span><br><span class="line">    public synchronized void say(String name) &#123;</span><br><span class="line">        System.out.println(&quot;say &quot; + name);</span><br><span class="line">        ThreadUtil.quietSleep(10000);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public synchronized void eat(String name) &#123;</span><br><span class="line">        System.out.println(&quot;eat &quot; + name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static synchronized void sayStatic(String name) &#123;</span><br><span class="line">        System.out.println(&quot;static say&quot; + name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static synchronized void eatStatic(String name) &#123;</span><br><span class="line">        System.out.println(&quot;static eat&quot; + name);</span><br><span class="line">        ThreadUtil.quietSleep(10000);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">package lock;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @author : haifeng.pang.</span><br><span class="line"> * @version 0.1 : Test v0.1 2021&#x2F;1&#x2F;28 下午5:49 By haifeng.pang.</span><br><span class="line"> * @description :</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class Test &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @org.junit.Test</span><br><span class="line">    public void test() &#123;</span><br><span class="line">        Human human &#x3D; new Human();</span><br><span class="line">        Human human1 &#x3D; new Human();</span><br><span class="line">        Thread threadA &#x3D; new Thread(new Runnable() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                human.say(&quot;im threadA&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        Thread threadB &#x3D; new Thread(new Runnable() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                human.eat(&quot;im threadB&quot;);</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        threadA.start();</span><br><span class="line"></span><br><span class="line">        threadB.start();</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            threadA.join();</span><br><span class="line">            threadB.join();</span><br><span class="line">        &#125;catch (Exception e) &#123;</span><br><span class="line">            &#x2F;&#x2F;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;挺简单的一个问题，今天被人问起了顺手写一下。&lt;/p&gt;
&lt;p&gt;其实 synchronized 是实例对象锁，而 static 是类对象锁。每个类类对象在一个classloader只会有一个。&lt;/p&gt;
&lt;p&gt;下面两段简单的代码就能验证&lt;/p&gt;
&lt;figure class=&quot;hi
      
    
    </summary>
    
    
      <category term="源码" scheme="http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="jdk" scheme="http://xingcici.github.io/tags/jdk/"/>
    
  </entry>
  
  <entry>
    <title>去中心化消息队列 Meta MQ</title>
    <link href="http://xingcici.github.io/2021/01/27/decentralized-message-queue-meta-mq.html"/>
    <id>http://xingcici.github.io/2021/01/27/decentralized-message-queue-meta-mq.html</id>
    <published>2021-01-27T12:25:24.885Z</published>
    <updated>2021-01-27T12:32:14.453Z</updated>
    
    <content type="html"><![CDATA[<p>虽然网上很多人说不要反复造轮子，但是就我个人的想法是造自己没造过的轮子是有利于技术的进步的。</p><p>最近时间稍微宽裕了一点，开始写一个去中心化的消息队列，暂时命名为 Meta MQ。</p><p>通过写这个消息队列，不断来深入理解业界目前消息队列的设计和原理，以及分布式协议，网络，存储方面的知识。</p><p>目前完成了最初步的功能，后面会不断得添砖加瓦~</p><p><a href="https://github.com/xingcici/meta-mq" target="_blank" rel="noopener">Meta MQ 地址</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;虽然网上很多人说不要反复造轮子，但是就我个人的想法是造自己没造过的轮子是有利于技术的进步的。&lt;/p&gt;
&lt;p&gt;最近时间稍微宽裕了一点，开始写一个去中心化的消息队列，暂时命名为 Meta MQ。&lt;/p&gt;
&lt;p&gt;通过写这个消息队列，不断来深入理解业界目前消息队列的设计和原理，以及
      
    
    </summary>
    
    
      <category term="中间件" scheme="http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="MQ" scheme="http://xingcici.github.io/tags/MQ/"/>
    
  </entry>
  
  <entry>
    <title>MR产出数据体积变大问题</title>
    <link href="http://xingcici.github.io/2021/01/27/mr-output-data-volume-becomes-larger.html"/>
    <id>http://xingcici.github.io/2021/01/27/mr-output-data-volume-becomes-larger.html</id>
    <published>2021-01-27T08:45:19.027Z</published>
    <updated>2021-01-27T09:04:17.617Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、现象"><a href="#一、现象" class="headerlink" title="一、现象"></a>一、现象</h3><p>前段时间在做数据同步任务的切换，在切换到MR后，总共36个任务，10个变小，16个不变，10个变大，而且变大的体积基本上都是翻倍。另外公司的存储空间当时较为紧张，所以需要解决这个体积增大的问题。</p><p>下图是用 parquet-tools 解析文件获得到，可以看到，左侧 outscan 比 右侧 mr 在好几个column上压缩率都高不少</p><p>ods_trd_buyer_order_app_order_desc_info_d</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165937262.png" alt="image-20210127165937262"></p><p>ods_itm_wd_inv_app_item_sku_d 则更加明显</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165912092.png" alt="image-20210127165912092"></p><h3 id="二、研究"><a href="#二、研究" class="headerlink" title="二、研究"></a>二、研究</h3><h4 id="1、outscan（之前的同步方式，下面不再注释）和mr同步的区别。"><a href="#1、outscan（之前的同步方式，下面不再注释）和mr同步的区别。" class="headerlink" title="1、outscan（之前的同步方式，下面不再注释）和mr同步的区别。"></a>1、outscan（之前的同步方式，下面不再注释）和mr同步的区别。</h4><table><thead><tr><th align="left">outscan</th><th align="left">mr</th></tr></thead><tbody><tr><td align="left">直接 map hfile，没有reduce过程</td><td align="left">分别map text 增量和 parquet 全量，再进行reduce</td></tr><tr><td align="left">输出类为 AvroParquetOutputFormat</td><td align="left">输出类为 ParquetOutputFormat</td></tr><tr><td align="left"></td><td align="left"></td></tr></tbody></table><h4 id="2、尝试调整-parquet-参数"><a href="#2、尝试调整-parquet-参数" class="headerlink" title="2、尝试调整 parquet 参数"></a>2、尝试调整 parquet 参数</h4><p>先放一张 parquet 文件格式图</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165049558.png" alt="image-20210127165049558"></p><p>可以看到 parquet 文件我们最需要关心的部分为 RowGroup、Column、 Page 。</p><p>当我们写 parquet 文件时，先会构造 RowGroup，然后根据不同的列构造不同的 Column，再分别在 Column 中构造 Page，然后向 Page 中写入数据，在每次写入时，会由 </p><p>accountForValueWritten() 判断是否需要 writePage ，如果需要的话会把当前 Column 中的 数据输出到一个 Collect，snappy 压缩也是在这个过程。当整个RowGroup到达整个 parquet.block.size 大小时，会把整个 RowGroup 输出到 hdfs。</p><table><thead><tr><th align="left">尝试调整的参数</th><th align="left">调整到的值</th><th align="left">对体积大小的影响</th></tr></thead><tbody><tr><td align="left">parquet.block.size</td><td align="left">1G</td><td align="left">基本无影响</td></tr><tr><td align="left"><code>parquet.page.size</code></td><td align="left">1-128M</td><td align="left">基本无影响</td></tr><tr><td align="left"><code>parquet.dictionary.page.size</code></td><td align="left">1-128M</td><td align="left">当只读 text 时，对体积约有20%的减小，在线上跑时，对体积基本无影响</td></tr><tr><td align="left"><code>io.compression.codec.snappy.buffersize</code></td><td align="left">1M</td><td align="left">基本无影响</td></tr><tr><td align="left"><code>io.file.buffer.size</code></td><td align="left">1M</td><td align="left">基本无影响</td></tr><tr><td align="left">输出类改为 AvroParquet</td><td align="left"></td><td align="left">基本无影响</td></tr><tr><td align="left">升级 parquet 版本</td><td align="left"></td><td align="left">基本无影响</td></tr><tr><td align="left"></td><td align="left"></td><td align="left"></td></tr></tbody></table><h4 id="3、查看-outscan-和-mr-的日志区别"><a href="#3、查看-outscan-和-mr-的日志区别" class="headerlink" title="3、查看 outscan 和 mr 的日志区别"></a>3、查看 outscan 和 mr 的日志区别</h4><h5 id="a-outscan"><a href="#a-outscan" class="headerlink" title="a) outscan"></a>a) outscan</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY</span><br><span class="line">Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728</span><br><span class="line">Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576</span><br><span class="line">Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576</span><br><span class="line">Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on</span><br><span class="line">Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off</span><br><span class="line">Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0</span><br><span class="line">Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134,279,407 &gt; 134,217,728: flushing 1,035,273 records to disk.</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134,097,175</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,122,796B for [auto_id] INT64: 1,035,273 values, 7,512,463B raw, 4,122,414B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 94,035 entries, 752,280B raw, 94,035B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,925,458B for [id] INT64: 1,035,273 values, 8,282,248B raw, 5,925,075B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE]</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,213,757B for [item_id] INT64: 1,035,273 values, 7,507,266B raw, 4,213,375B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 83,192 entries, 665,536B raw, 83,192B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,449,023B for [price] BINARY: 1,035,273 values, 1,548,410B raw, 1,448,622B comp, 9 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 13,877 entries, 135,544B raw, 13,877B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 305,529B for [quantity] INT64: 1,035,273 values, 601,051B raw, 305,153B comp, 8 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1,455 entries, 11,640B raw, 1,455B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,585,004B for [total_price] BINARY: 1,035,273 values, 1,726,198B raw, 1,584,599B comp, 9 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 33,741 entries, 334,219B raw, 33,741B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 25,257B for [cps_fee] BINARY: 1,035,273 values, 50,010B raw, 24,968B comp, 8 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 314 entries, 2,627B raw, 314B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,715,269B for [item_sku_id] INT64: 1,035,273 values, 6,616,763B raw, 2,714,888B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 104,273 entries, 834,184B raw, 104,273B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,605,807B for [add_time] BINARY: 1,035,273 values, 22,843,179B raw, 6,604,199B comp, 23 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 27,693 entries, 636,939B raw, 27,693B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,098,234B for [order_id] INT64: 1,035,273 values, 7,490,983B raw, 4,097,852B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 76,367 entries, 610,936B raw, 76,367B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 402B for [sk] BINARY: 1,035,273 values, 94B raw, 102B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 7 entries, 392B raw, 7B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,835,688B for [item_sku_title] BINARY: 1,035,273 values, 12,055,000B raw, 5,834,969B comp, 13 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 23,553 entries, 567,428B raw, 23,553B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 34,929,921B for [item_title] BINARY: 1,035,273 values, 67,065,375B raw, 34,918,001B comp, 65 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 9,636 entries, 720,750B raw, 9,636B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 23,697,146B for [img_head] BINARY: 1,035,273 values, 62,188,305B raw, 23,689,804B comp, 61 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 11,114 entries, 692,307B raw, 11,114B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,024,817B for [extend] BINARY: 1,035,273 values, 59,829,153B raw, 15,022,915B comp, 56 pages, encodings: [BIT_PACKED, PLAIN, RLE]</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 456B for [data_ver] INT64: 1,035,273 values, 96B raw, 112B comp, 8 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1 entries, 8B raw, 1B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,496,780B for [update_time] BINARY: 1,035,273 values, 22,843,365B raw, 6,495,172B comp, 23 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 27,530 entries, 633,190B raw, 27,530B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 486,705B for [buyer_id] BINARY: 1,035,273 values, 1,757,571B raw, 486,033B comp, 14 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 72,340 entries, 980,153B raw, 72,340B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,248,275B for [seller_id] INT32: 1,035,273 values, 1,440,252B raw, 1,248,119B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 112,471 entries, 449,884B raw, 112,471B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 164,710B for [f_seller_id] INT32: 1,035,273 values, 251,370B raw, 164,554B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 12,392 entries, 49,568B raw, 12,392B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 149,608B for [status] INT32: 1,035,273 values, 158,031B raw, 149,452B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 3 entries, 12B raw, 3B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 456B for [rate_fund] INT64: 1,035,273 values, 96B raw, 112B comp, 8 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1 entries, 8B raw, 1B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 169,390B for [version_id] INT32: 1,035,273 values, 231,460B raw, 169,234B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 13 entries, 52B raw, 13B comp&#125;</span><br><span class="line">Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67,464B for [refund_status] INT32: 1,035,273 values, 89,951B raw, 67,308B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 4 entries, 16B raw, 4B comp&#125;</span><br></pre></td></tr></table></figure><h5 id="b-mr"><a href="#b-mr" class="headerlink" title="b) mr"></a>b) mr</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY</span><br><span class="line">Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728</span><br><span class="line">Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576</span><br><span class="line">Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576</span><br><span class="line">Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on</span><br><span class="line">Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off</span><br><span class="line">Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0</span><br><span class="line">Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes</span><br><span class="line">Dec 11, 2020 12:10:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134,596,338 &gt; 134,217,728: flushing 679,290 records to disk.</span><br><span class="line">Dec 11, 2020 12:10:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 135,665,494</span><br><span class="line">Dec 11, 2020 12:10:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 84,603B for [auto_id] INT64: 679,290 values, 84,384B raw, 84,323B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 22,331 entries, 178,648B raw, 22,331B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,121,794B for [id] INT64: 679,290 values, 5,434,368B raw, 5,121,507B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN]</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,114,386B for [item_id] INT64: 679,290 values, 5,434,368B raw, 4,114,099B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN]</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,140,171B for [price] BINARY: 679,290 values, 1,158,473B raw, 1,139,909B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 12,680 entries, 123,547B raw, 12,680B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 281,422B for [quantity] INT64: 679,290 values, 643,817B raw, 281,140B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1,213 entries, 9,704B raw, 1,213B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,225,975B for [total_price] BINARY: 679,290 values, 1,243,295B raw, 1,225,712B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 27,429 entries, 269,122B raw, 27,429B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,977B for [cps_fee] BINARY: 679,290 values, 2,390B raw, 1,760B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 165 entries, 1,357B raw, 165B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,572,263B for [item_sku_id] INT64: 679,290 values, 4,662,155B raw, 2,571,977B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 70,731 entries, 565,848B raw, 70,731B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,933,270B for [add_time] BINARY: 679,290 values, 15,623,790B raw, 6,932,221B comp, 15 pages, encodings: [BIT_PACKED, RLE, PLAIN]</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,037,909B for [order_id] INT64: 679,290 values, 5,434,368B raw, 5,037,622B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN]</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 332B for [sk] BINARY: 679,290 values, 117B raw, 123B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 9 entries, 548B raw, 9B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,675,428B for [item_sku_title] BINARY: 679,290 values, 7,868,590B raw, 5,674,984B comp, 9 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 30,819 entries, 729,235B raw, 30,819B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 34,917,594B for [item_title] BINARY: 679,290 values, 41,284,243B raw, 34,910,519B comp, 41 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 15,403 entries, 1,011,183B raw, 15,403B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 27,409,350B for [img_head] BINARY: 679,290 values, 41,769,032B raw, 27,405,580B comp, 41 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 15,612 entries, 1,014,148B raw, 15,612B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 16,932,320B for [extend] BINARY: 679,290 values, 42,638,909B raw, 16,930,559B comp, 42 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 17,372 entries, 1,008,909B raw, 17,372B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 342B for [data_ver] INT64: 679,290 values, 72B raw, 84B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1 entries, 8B raw, 1B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,800,709B for [update_time] BINARY: 679,290 values, 15,623,790B raw, 6,799,660B comp, 15 pages, encodings: [BIT_PACKED, RLE, PLAIN]</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,194,701B for [buyer_id] BINARY: 679,290 values, 9,200,418B raw, 6,194,344B comp, 9 pages, encodings: [BIT_PACKED, RLE, PLAIN]</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,464,540B for [seller_id] INT32: 679,290 values, 1,464,273B raw, 1,464,423B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 131,083 entries, 524,332B raw, 131,083B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 295,854B for [f_seller_id] INT32: 679,290 values, 550,175B raw, 295,737B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 16,979 entries, 67,916B raw, 16,979B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 152,287B for [status] INT32: 679,290 values, 161,460B raw, 152,170B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 3 entries, 12B raw, 3B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 342B for [rate_fund] INT64: 679,290 values, 72B raw, 84B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1 entries, 8B raw, 1B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 226,851B for [version_id] INT32: 679,290 values, 309,556B raw, 226,734B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 16 entries, 64B raw, 16B comp&#125;</span><br><span class="line">Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65,837B for [refund_status] INT32: 679,290 values, 101,856B raw, 65,720B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 4 entries, 16B raw, 4B comp&#125;</span><br></pre></td></tr></table></figure><p>可以看到 parquet.block.size 、page.size 、 dictionary.page.size 这些参数都一样的情况下，在一个 rowgroup 中 mr 能写入的数据少于 outscan 能写入的数据，这就是因为压缩率不同导致的。</p><h4 id="3、查看源码"><a href="#3、查看源码" class="headerlink" title="3、查看源码"></a>3、查看源码</h4><h5 id="a-snappy"><a href="#a-snappy" class="headerlink" title="a) snappy"></a>a) snappy</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.parquet.hadoop.CodecFactory.BytesCompressor#compress  public BytesInput compress(BytesInput bytes) throws IOException &#123;  final BytesInput compressedBytes;  if (codec &#x3D;&#x3D; null) &#123;    compressedBytes &#x3D; bytes;  &#125; else &#123;    compressedOutBuffer.reset();    if (compressor !&#x3D; null) &#123;      &#x2F;&#x2F; null compressor for non-native gzip      compressor.reset();    &#125;    CompressionOutputStream cos &#x3D; codec.createOutputStream(compressedOutBuffer, compressor);    bytes.writeAllTo(cos);    cos.finish();    cos.close();    compressedBytes &#x3D; BytesInput.from(compressedOutBuffer);  &#125;  return compressedBytes; &#125;  org.apache.parquet.hadoop.codec.SnappyCompressor#compress  @Override public synchronized int compress(byte[] buffer, int off, int len) throws IOException &#123;  SnappyUtil.validateBuffer(buffer, off, len);   if (needsInput()) &#123;    &#x2F;&#x2F; No buffered output bytes and no input to consume, need more input    return 0;  &#125;   if (!outputBuffer.hasRemaining()) &#123;    &#x2F;&#x2F; There is uncompressed input, compress it now    int maxOutputSize &#x3D; Snappy.maxCompressedLength(inputBuffer.position());    if (maxOutputSize &gt; outputBuffer.capacity()) &#123;      outputBuffer &#x3D; ByteBuffer.allocateDirect(maxOutputSize);    &#125;    &#x2F;&#x2F; Reset the previous outputBuffer    outputBuffer.clear();    inputBuffer.limit(inputBuffer.position());    inputBuffer.position(0);     int size &#x3D; Snappy.compress(inputBuffer, outputBuffer);    outputBuffer.limit(size);    inputBuffer.limit(0);    inputBuffer.rewind();  &#125;   &#x2F;&#x2F; Return compressed output up to &#39;len&#39;  int numBytes &#x3D; Math.min(len, outputBuffer.remaining());  outputBuffer.get(buffer, off, numBytes);      bytesWritten +&#x3D; numBytes;  return numBytes;      &#125;  org.xerial.snappy.Snappy#compress(java.nio.ByteBuffer, java.nio.ByteBuffer)  public static int compress(ByteBuffer uncompressed, ByteBuffer compressed) throws IOException &#123;     if (!uncompressed.isDirect())        throw new SnappyError(SnappyErrorCode.NOT_A_DIRECT_BUFFER, &quot;input is not a direct buffer&quot;);    if (!compressed.isDirect())        throw new SnappyError(SnappyErrorCode.NOT_A_DIRECT_BUFFER, &quot;destination is not a direct buffer&quot;);     &#x2F;&#x2F; input: uncompressed[pos(), limit())    &#x2F;&#x2F; output: compressed    int uPos &#x3D; uncompressed.position();    int uLen &#x3D; uncompressed.remaining();    int compressedSize &#x3D; ((SnappyNativeAPI) impl).rawCompress(uncompressed, uPos, uLen, compressed,            compressed.position());     &#x2F;&#x2F;         pos  limit    &#x2F;&#x2F; [ ......BBBBBBB.........]    compressed.limit(compressed.position() + compressedSize);     return compressedSize; &#125;</span><br></pre></td></tr></table></figure><p>上面是 snappy 压缩的过程，可以看到 snappy 对输入的流除了不能超过 Integer.MAXVALUE，其他没有做什么限制。也就是输入什么，它就按它的逻辑去压缩什么。parquet 获取到压缩后的流写入内存 page 中。</p><p>因此推测问题应该不在 snappy 这边。</p><h5 id="b-parquet"><a href="#b-parquet" class="headerlink" title="b) parquet"></a>b) parquet</h5><p>parquet 源码我就不复制过来了，有点多。 parquet 也只是获取到 reduce 的输出流，对数据根据类型和内容进行不同的编码，然后形成它的数据结构再用 snappy 进行压缩，再输出到文件。</p><p>需要注意的是 parquet.dictionary.page.size 这个参数会对编码造成一定影响，当 dictionary 里的值大于配置的值，编码会退化。</p><h5 id="c-mapreduce"><a href="#c-mapreduce" class="headerlink" title="c) mapreduce"></a>c) mapreduce</h5><p>重点怀疑是在 reduce 后或者过程中，输出数据之间较为稀疏，导致压缩效率不理想。但是这块不是非常熟悉，需要找方法验证。</p><h3 id="三、结果"><a href="#三、结果" class="headerlink" title="三、结果"></a>三、结果</h3><p>已经找到原因，确实是 reduce 后数据较为稀疏导致压缩率不高。</p><p>前天在排查的时候，发现 outscan 出来的数据某些 ID 都是连续的，而后面的 title 数据会有基本相同的数据排在一起。怀疑某些表的数据有特殊性，在 ID 连续时，其他列的数据基本相同。后续也证实了这个猜想。</p><p>当某些表的 xxx_id 连续分布在同一个 reduce 中时，会对压缩率有极大的提高，例如</p><p>| item_sku_id| title |<br>| 11231 | 黑色羊毛衫XL|<br>| 11232 | 黑色羊毛衫XXL|</p><p>| 11233 | 遥控汽车A |</p><p>| 11234 | 遥控汽车B |</p><p> 默认的 partition 会将 11231，11233发送到 reduce A , 11232 ，11234 发送到 reduce B,最后产生两个内容基本没有重复的文件，会导致数据不连续 压缩率不理想。</p><p>而 outscan 没有 reduce 的过程，直接 map 完有序输出，不会将连续的 id 切分到各个 reduce。</p><p>解决办法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public int getPartition(Text text, Text value, int numPartitions) &#123;    long key &#x3D; Long.parseLong((text.toString().split(&quot;_&quot;))[0]);    return (int) (key&#x2F;100%numPartitions); &#125;</span><br></pre></td></tr></table></figure><p>增加了一个 partition，专门给这些数据上有连续性的表用，将连续的 ID 发送到同一个 reduce 输出。sub最后两位是观察我们DB里的数据的连续性得出的经验值。</p><h3 id="四、解决问题之外的研究"><a href="#四、解决问题之外的研究" class="headerlink" title="四、解决问题之外的研究"></a>四、解决问题之外的研究</h3><p>最后研究下 parquet 的编码对 parquet 本身压缩率有多大的影响。</p><p>开启 SNAPPY 压缩，自定义分区方式重跑 ods_itm_wd_inv_app_item_sku_d 任务结果体积大小及编码结果</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165125612.png" alt="image-20210127165125612"></p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165142007.png" alt="image-20210127165142007"></p><p>关闭SNAPPY压缩后，自定义分区方式重新跑 ods_itm_wd_inv_app_item_sku_d 任务结果体积大小及编码结果。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165848742.png" alt="image-20210127165848742"></p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165221964.png" alt="image-20210127165221964"></p><p>关闭SNAPPY压缩后，默认分区方式重新跑 ods_itm_wd_inv_app_item_sku_d 任务结果体积大小及编码结果。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165830347.png" alt="image-20210127165830347"></p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165246366.png" alt="image-20210127165246366"></p><p>根据上面的数据可以得出，在数据密集的情况下，parquet 本身对数据的压缩率就会高一些，在数据不密集的情况下 5.1T ，数据密集的情况下 3.7T。</p><p>下面两张图可以看到，有序和无序对都是字符的 title 列的编码方式影响不大，都用PLAIN编码。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165814330.png" alt="image-20210127165814330"></p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20210127165310098.png" alt="image-20210127165310098"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、现象&quot;&gt;&lt;a href=&quot;#一、现象&quot; class=&quot;headerlink&quot; title=&quot;一、现象&quot;&gt;&lt;/a&gt;一、现象&lt;/h3&gt;&lt;p&gt;前段时间在做数据同步任务的切换，在切换到MR后，总共36个任务，10个变小，16个不变，10个变大，而且变大的体积基本上都是
      
    
    </summary>
    
    
      <category term="数据智能" scheme="http://xingcici.github.io/categories/%E6%95%B0%E6%8D%AE%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="hadoop" scheme="http://xingcici.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Skywalking 6.6.0对arthas的兼容性问题</title>
    <link href="http://xingcici.github.io/2020/09/10/skywalking-660-compatibility-issues-with-arthas.html"/>
    <id>http://xingcici.github.io/2020/09/10/skywalking-660-compatibility-issues-with-arthas.html</id>
    <published>2020-09-10T09:08:08.549Z</published>
    <updated>2020-09-11T03:29:25.141Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>目前在参与公司全链路压测项目，在部署到日常环境服务进行测试时，用 arthas 进行追踪排查问题发现无法追踪被 agent 增强的类。于是尝试解决这个问题。</p><p>报错如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;arthas trace 时命令行报错</span><br><span class="line">[arthas@24580]$ trace com.mysql.jdbc.PreparedStatement execute</span><br><span class="line">Affect(class count: 1 , method count: 5) cost in 480 ms, listenerId: 8</span><br><span class="line">Enhance error! exception: java.lang.UnsupportedOperationException: class redefinition failed: attempted to change superclass or interfaces</span><br><span class="line">error happens when enhancing class: class redefinition failed: attempted to change superclass or interfaces, check arthas log: &#x2F;home&#x2F;www&#x2F;logs&#x2F;arthas&#x2F;arthas.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;arthas 日志报错</span><br><span class="line">2020-09-10 17:05:24 [arthas-command-execute] INFO  c.t.arthas.core.advisor.Enhancer -enhance matched classes: [class com.mysql.jdbc.PreparedStatement, class com.mysql.jdbc.JDBC4PreparedStatement, class com.mysql.jdbc.CallableStatement, class com.mysql.jdbc.ServerPreparedStatement]</span><br><span class="line">2020-09-10 17:05:24 [arthas-command-execute] ERROR c.t.arthas.core.advisor.Enhancer -Enhancer error, matchingClasses: [class com.mysql.jdbc.PreparedStatement, class com.mysql.jdbc.JDBC4PreparedStatement, class com.mysql.jdbc.CallableStatement, class com.mysql.jdbc.ServerPreparedStatement]</span><br><span class="line">java.lang.UnsupportedOperationException: class redefinition failed: attempted to change superclass or interfaces</span><br><span class="line">at sun.instrument.InstrumentationImpl.retransformClasses0(Native Method)</span><br><span class="line">at sun.instrument.InstrumentationImpl.retransformClasses(InstrumentationImpl.java:144)</span><br><span class="line">at com.taobao.arthas.core.advisor.Enhancer.enhance(Enhancer.java:368)</span><br><span class="line">at com.taobao.arthas.core.command.monitor200.EnhancerCommand.enhance(EnhancerCommand.java:149)</span><br><span class="line">at com.taobao.arthas.core.command.monitor200.EnhancerCommand.process(EnhancerCommand.java:96)</span><br><span class="line">at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl.process(AnnotatedCommandImpl.java:82)</span><br><span class="line">at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl.access$100(AnnotatedCommandImpl.java:18)</span><br><span class="line">at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl$ProcessHandler.handle(AnnotatedCommandImpl.java:111)</span><br><span class="line">at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl$ProcessHandler.handle(AnnotatedCommandImpl.java:108)</span><br><span class="line">at com.taobao.arthas.core.shell.system.impl.ProcessImpl$CommandProcessTask.run(ProcessImpl.java:385)</span><br><span class="line">at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)</span><br><span class="line">at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br></pre></td></tr></table></figure><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><p>首先 skywalking 和 arthas 都是目前比较受欢迎的开源项目，我们应该不是第一个碰到这个问题的人。Github 一搜，果然不少人都碰到了这个问题（部分资料见参考资料）。</p><p>其中 arthas 维护团队的一位成员已经对 skywalking 提出了 pr，并已经合并到 matser，但是我们是 6.6.0，master 已经是 8.x。只能参考下进行改造。</p><p>基本的改造点为：</p><ol><li>开源代码中的方案包括了 文件 和 内存缓存两种方式，我们不需要文件这种比较重量级的方式，于是去掉了，而且将内存的方式改为了LRU。</li><li>6.6.0 加载自定义 CacheableTransformerDecorator 的方式和 8.x 不太一样，主要是 bytebuddy 版本差异造成的。6.6.0 加载的地方为 agentBuilder.installOn(x,x) 。</li></ol><p>这样改造后基本就能兼容 arthas 了。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><p>[skywalking 6.4.0 版本兼容性]:<a href="https://github.com/alibaba/arthas/issues/898" target="_blank" rel="noopener">https://github.com/alibaba/arthas/issues/898</a></p></li><li><p>[support class cache for ByteBuddy]:<a href="https://github.com/apache/skywalking/pull/4858/files" target="_blank" rel="noopener">https://github.com/apache/skywalking/pull/4858/files</a></p></li><li><p>[How to advice a class that have been intercepted by another javaagent but not loaded]: <a href="https://github.com/raphw/byte-buddy/issues/829" target="_blank" rel="noopener">https://github.com/raphw/byte-buddy/issues/829</a></p></li><li><p>[Easily Create Java Agents with Byte Buddy]:<a href="https://www.infoq.com/articles/Easily-Create-Java-Agents-with-ByteBuddy/" target="_blank" rel="noopener">https://www.infoq.com/articles/Easily-Create-Java-Agents-with-ByteBuddy/</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;目前在参与公司全链路压测项目，在部署到日常环境服务进行测试时，用 arthas 进行追踪排查问题发现无法追踪被 agent 增强的类。于是尝
      
    
    </summary>
    
    
      <category term="中间件" scheme="http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="链路追踪" scheme="http://xingcici.github.io/tags/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/"/>
    
  </entry>
  
  <entry>
    <title>零知识证明</title>
    <link href="http://xingcici.github.io/2020/07/13/zero-knowledge-proof.html"/>
    <id>http://xingcici.github.io/2020/07/13/zero-knowledge-proof.html</id>
    <published>2020-07-13T10:58:34.892Z</published>
    <updated>2020-07-13T11:20:42.193Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/29491567" target="_blank" rel="noopener">零知识证明：一个略微严肃的科普</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/29491567&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;零知识证明：一个略微严肃的科普&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="杂七杂八" scheme="http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
      <category term="密码学" scheme="http://xingcici.github.io/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>FutureTask</title>
    <link href="http://xingcici.github.io/2020/07/13/futuretask.html"/>
    <id>http://xingcici.github.io/2020/07/13/futuretask.html</id>
    <published>2020-07-13T09:44:52.627Z</published>
    <updated>2020-07-13T09:47:11.607Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>我们实现 Callable 接口，在覆写的 call 方法中定义需要执行的业务逻辑；</p></li><li><p>然后把我们实现的 Callable 接口实现对象传给 FutureTask，然后 FutureTask 作为异步任务提交给线程执行；</p></li><li><p>最重要的是 FutureTask 内部维护了一个状态 state，任何操作（异步任务正常结束与否还是被取消）都是围绕着这个状态进行，并随时更新 state 任务的状态；</p></li><li><p>只能有一个线程执行异步任务，当异步任务执行结束后，此时可能正常结束，异常结束或被取消。</p></li><li><p>可以多个线程并发获取异步任务执行结果，当异步任务还未执行完，此时获取异步任务的线程将加入线程等待列表进行等待；</p></li><li><p>当异步任务线程执行结束后，此时会唤醒获取异步任务执行结果的线程，注意唤醒顺序是 “后进先出” 即后面加入的阻塞线程先被唤醒。</p></li><li><p>当我们调用 FutureTask.cancel 方法时并不能真正停止执行异步任务的线程，只是发出中断线程的信号。但是只要 cancel 方法返回 true，此时即使异步任务能正常执行完，此时我们调用 get 方法获取结果时依然会抛出 CancellationException 异常。</p></li></ol><ol><li>利用 LockSupport 来实现线程的阻塞 \ 唤醒机制；</li><li>利用 volatile 和 UNSAFE 的 CAS 方法来实现线程共享变量的无锁化操作；</li><li>若要编写超时异常的逻辑可以参考 FutureTask 的 get(long timeout, TimeUnit unit) 的实现逻辑；</li><li>多线程获取某一成员变量结果时若需要等待时的线程等待链表的逻辑实现；</li><li>某一异步任务在某一时刻只能由单一线程执行的逻辑实现；</li><li>FutureTask 中的任务状态 state 的变化处理的逻辑实现。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;p&gt;我们实现 Callable 接口，在覆写的 call 方法中定义需要执行的业务逻辑；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;然后把我们实现的 Callable 接口实现对象传给 FutureTask，然后 FutureTask 作为异步任务提交给线程执行；&lt;/
      
    
    </summary>
    
    
      <category term="源码" scheme="http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="JDK" scheme="http://xingcici.github.io/tags/JDK/"/>
    
  </entry>
  
  <entry>
    <title>RockeMQ 单机版在 centos 上的部署</title>
    <link href="http://xingcici.github.io/2020/06/30/rockemq-standalone-version-deployment-on-centos.html"/>
    <id>http://xingcici.github.io/2020/06/30/rockemq-standalone-version-deployment-on-centos.html</id>
    <published>2020-06-30T10:14:15.737Z</published>
    <updated>2020-07-01T05:34:11.679Z</updated>
    
    <content type="html"><![CDATA[<p>最近需要需要再研究下 RocketMQ 的文件系统的具体实现，于是重新在自己的服务器上安装了一遍，记录下过程。</p><ol><li><p>到镜像站下载安装包 wget <a href="http://mirror.bit.edu.cn/apache/rocketmq/4.7.1/rocketmq-all-4.7.1-bin-release.zip" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/rocketmq/4.7.1/rocketmq-all-4.7.1-bin-release.zip</a> 并解压缩</p></li><li><p>修改 broker.conf 文件（conf/broker.conf）添加 naemServer 地址的属性以及自动创建 Topic 的属性。</p><blockquote><p>namesrvAddr = 127.0.0.1:9876</p><p>autoCreateTopicEnable = true</p></blockquote></li><li><p>修改启动参数（由于 rocketMQ 对内存的消耗比较大，所以测试时修改为本机合适的大小）。主要修改 bin 目录下的 runserver.sh 和 runbroker.sh 下的 JAVA_OPT 属性。</p></li><li><p>nohup sh bin/mqnamesrv &amp;</p><p>nohup sh bin/mqbroker -c /root/download/rocketmq/conf/broker.conf &amp;</p><p>sh bin/mqshutdown namesrv</p><p>sh bin/mqshutdown broker</p></li><li><p>注意开放端口</p></li></ol><p>有几个坑要注意</p><ol><li><p>开放端口 </p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200701133221711.png" alt="image-20200701133221711"></p></li><li><p>我在部署的时候碰到 store 目录下没法自动建文件夹的问题。</p></li><li><p>broker.conf 增加配置</p><blockquote><p>namesrvAddr = 127.0.0.1:9876<br>brokerIP1=47.101.33.17<br>autoCreateTopicEnable = true</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近需要需要再研究下 RocketMQ 的文件系统的具体实现，于是重新在自己的服务器上安装了一遍，记录下过程。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;到镜像站下载安装包 wget &lt;a href=&quot;http://mirror.bit.edu.cn/apache/rocketmq/
      
    
    </summary>
    
    
      <category term="中间件" scheme="http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="RocketMQ" scheme="http://xingcici.github.io/tags/RocketMQ/"/>
    
  </entry>
  
  <entry>
    <title>反射获取一个类的私有方法</title>
    <link href="http://xingcici.github.io/2020/06/24/reflection-to-get-a-class-private-method.html"/>
    <id>http://xingcici.github.io/2020/06/24/reflection-to-get-a-class-private-method.html</id>
    <published>2020-06-24T09:29:25.561Z</published>
    <updated>2020-06-30T03:33:21.715Z</updated>
    
    <content type="html"><![CDATA[<p>比较简单</p><p><a href="https://github.com/xingcici/learn-demo/tree/master/reflect" target="_blank" rel="noopener">Github</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AccessPrivateMember</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class c = Class.forName(<span class="string">"com.example.reflect.HelloService"</span>);</span><br><span class="line">            <span class="comment">//能获取所有有访问权限的方法，包括父类中继承的</span></span><br><span class="line">            Method publicMethod = c.getMethod(<span class="string">"publicHello"</span>, String<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">            Method saySomething = c.getMethod(<span class="string">"publicHello"</span>, String<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">            <span class="comment">//获取所有方法 本方法中</span></span><br><span class="line">            Method thisClassMethod = c.getDeclaredMethod(<span class="string">"privateHello"</span>, String<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">            <span class="comment">//设置权限</span></span><br><span class="line">            thisClassMethod.setAccessible(<span class="keyword">true</span>);</span><br><span class="line"><span class="comment">//不能直接转成类来执行 classloader 不同</span></span><br><span class="line">            publicMethod.invoke(c.newInstance(), <span class="string">"publicMethod"</span>);</span><br><span class="line">            saySomething.invoke(c.newInstance(), <span class="string">"saySomething"</span>);</span><br><span class="line"></span><br><span class="line">            thisClassMethod.invoke(c.newInstance(), <span class="string">"thisClassMethod"</span>);</span><br><span class="line"></span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;比较简单&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/xingcici/learn-demo/tree/master/reflect&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;
&lt;figure cl
      
    
    </summary>
    
    
      <category term="编程" scheme="http://xingcici.github.io/categories/%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="反射" scheme="http://xingcici.github.io/tags/%E5%8F%8D%E5%B0%84/"/>
    
  </entry>
  
  <entry>
    <title>RocketMQ 消息存储的设计与实现</title>
    <link href="http://xingcici.github.io/2020/06/24/design-and-implementation-of-rocketmq-message-storage.html"/>
    <id>http://xingcici.github.io/2020/06/24/design-and-implementation-of-rocketmq-message-storage.html</id>
    <published>2020-06-24T09:01:25.741Z</published>
    <updated>2020-07-01T07:10:45.920Z</updated>
    
    <content type="html"><![CDATA[<p>作为一款高性能的消息中间件，RocketMQ 基于互联网的生产要求对多 Topic 场景做了诸多针对性优化。根据中间件团队提供的压测报告，在 Producer 和 Consumer 共存的情况下，相比于 Kafka，RocketMQ 的性能指标（TPS 和 RT）随着 Topic 数量的上升表现稳定。本文从消息存储的角度谈谈 RocketMQ 高性能的原因，重点包括四个方面：消息文件存储的结构设计、消息从 Broker 接收到持久化磁盘的流程、刷盘策略和内存映射优化机制。</p><h3 id="消息文件存储结构设计"><a href="#消息文件存储结构设计" class="headerlink" title="消息文件存储结构设计"></a>消息文件存储结构设计</h3><h3 id="消息持久化"><a href="#消息持久化" class="headerlink" title="消息持久化"></a>消息持久化</h3><h3 id="刷盘策略"><a href="#刷盘策略" class="headerlink" title="刷盘策略"></a>刷盘策略</h3><h3 id="内存映射"><a href="#内存映射" class="headerlink" title="内存映射"></a>内存映射</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作为一款高性能的消息中间件，RocketMQ 基于互联网的生产要求对多 Topic 场景做了诸多针对性优化。根据中间件团队提供的压测报告，在 Producer 和 Consumer 共存的情况下，相比于 Kafka，RocketMQ 的性能指标（TPS 和 RT）随着 To
      
    
    </summary>
    
    
      <category term="中间件" scheme="http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="RocketMQ" scheme="http://xingcici.github.io/tags/RocketMQ/"/>
    
  </entry>
  
  <entry>
    <title>深入理解 JAVA 反序列化漏洞</title>
    <link href="http://xingcici.github.io/2020/06/24/indepth-understanding-of-java-deserialization-vulnerability.html"/>
    <id>http://xingcici.github.io/2020/06/24/indepth-understanding-of-java-deserialization-vulnerability.html</id>
    <published>2020-06-24T01:47:01.053Z</published>
    <updated>2020-06-24T06:59:04.258Z</updated>
    
    <content type="html"><![CDATA[<p>惊闻  dubbo 爆出严重的反序列化漏洞 <a href="https://www.anquanke.com/post/id/209102" target="_blank" rel="noopener">CVE-2020-1948：Apache Dubbo 远程代码执行漏洞通告</a></p><p>加上之前的 fastjson 不断的反序列化漏洞，于是了解了下这方面的知识。详见<a href="https://paper.seebug.org/312/" target="_blank" rel="noopener">深入理解 JAVA 反序列化漏洞</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;惊闻  dubbo 爆出严重的反序列化漏洞 &lt;a href=&quot;https://www.anquanke.com/post/id/209102&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVE-2020-1948：Apache Dubbo 远程代码执行漏
      
    
    </summary>
    
    
      <category term="杂七杂八" scheme="http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux命令学习</title>
    <link href="http://xingcici.github.io/2020/06/23/linux-command-learning.html"/>
    <id>http://xingcici.github.io/2020/06/23/linux-command-learning.html</id>
    <published>2020-06-23T06:16:36.652Z</published>
    <updated>2020-06-24T06:59:04.203Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Linux命令大全"><a href="#Linux命令大全" class="headerlink" title="Linux命令大全"></a><a href="https://www.runoob.com/linux/linux-command-manual.html" target="_blank" rel="noopener">Linux命令大全</a></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Linux命令大全&quot;&gt;&lt;a href=&quot;#Linux命令大全&quot; class=&quot;headerlink&quot; title=&quot;Linux命令大全&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://www.runoob.com/linux/linux-command-manual.
      
    
    </summary>
    
    
      <category term="杂七杂八" scheme="http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
      <category term="linux" scheme="http://xingcici.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>编程中的一些感悟</title>
    <link href="http://xingcici.github.io/2020/06/18/some-sentiments-in-programming.html"/>
    <id>http://xingcici.github.io/2020/06/18/some-sentiments-in-programming.html</id>
    <published>2020-06-18T12:20:02.180Z</published>
    <updated>2020-06-24T06:59:04.236Z</updated>
    
    <content type="html"><![CDATA[<h3 id="加强对象意识"><a href="#加强对象意识" class="headerlink" title="加强对象意识"></a>加强对象意识</h3><p>之前虽然有意识在强化，但是有时候还是写出简单的命令式的代码。对同一个事物的一系列操作，基本都能以该事物发起操作的形式来设计，包装一下。</p><h4 id="lifecycle"><a href="#lifecycle" class="headerlink" title="lifecycle"></a>lifecycle</h4><p>最近开始真的写中间件才意识到，对对象的生命周期的掌握非常重要，下面的这种形式就很方便</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Server</span> <span class="keyword">implements</span> <span class="title">LifeCycle</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">      &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shutdown</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;加强对象意识&quot;&gt;&lt;a href=&quot;#加强对象意识&quot; class=&quot;headerlink&quot; title=&quot;加强对象意识&quot;&gt;&lt;/a&gt;加强对象意识&lt;/h3&gt;&lt;p&gt;之前虽然有意识在强化，但是有时候还是写出简单的命令式的代码。对同一个事物的一系列操作，基本都能以该事物发起操
      
    
    </summary>
    
    
      <category term="架构设计" scheme="http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
  </entry>
  
  <entry>
    <title>Sofa-bolt的简单使用</title>
    <link href="http://xingcici.github.io/2020/06/18/simple-use-of-sofabolt.html"/>
    <id>http://xingcici.github.io/2020/06/18/simple-use-of-sofabolt.html</id>
    <published>2020-06-18T03:48:20.319Z</published>
    <updated>2020-06-24T06:59:04.218Z</updated>
    
    <content type="html"><![CDATA[<p>代码已经放在 <a href="https://github.com/xingcici/xc-raft/tree/bolt-basic" target="_blank" rel="noopener">github</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;代码已经放在 &lt;a href=&quot;https://github.com/xingcici/xc-raft/tree/bolt-basic&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;github&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="中间件" scheme="http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="网络" scheme="http://xingcici.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Netty" scheme="http://xingcici.github.io/tags/Netty/"/>
    
  </entry>
  
  <entry>
    <title>由No route info for this topic引发的关于RocketMQ的问题</title>
    <link href="http://xingcici.github.io/2020/06/17/questions-about-rocketmq-caused-by-no-route-info-for-this-topic.html"/>
    <id>http://xingcici.github.io/2020/06/17/questions-about-rocketmq-caused-by-no-route-info-for-this-topic.html</id>
    <published>2020-06-17T01:21:44.351Z</published>
    <updated>2020-06-24T06:59:04.329Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>昨天一位业务的同学在我审批MQ通过后，在业务代码里加了个 producer，结果启动项目时集成的调度中心的二方包里的 producer 在通过 MQ 注册 JOB 时报 No route info for this topic。</p><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>这个异常也算比较常见了，一般是没有创建 topic，或者连接错了 namesrv 导致。但是查看了他的代码后发现配置<code>没有问题</code>。然后他反馈他 DEBUG 出来 producer 里的 namesrv 是正确的，然后他自己新加的。这就很奇怪了，于是我让他提交了代码然后申请了代码权限看下原因。</p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>这里先说一句，我对 BUG 这种东西，一直都相信一句话，99%你碰到觉得无比高深的 BUG，基本都是非常简单的点弄错了导致的。这次果然还是如此。</p><p>一开始我都没去看配置的问题，因为从业务方之前截图给我的配置里是没问题的。我们对配置的引用有两种方式，一种是 @key@，另外一种是 ${key}。截图给我的时候用的是 @key@，我想当然觉得这应该也是能正常拿到 value 的。</p><p>于是开始DEBUG，发现他配置的 @key@ 居然没引用到 value，直接把 @key @作为 namesrv 传进去了,如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200617094655941.png" alt="image-20200617094655941"></p><p>但是思考一下，又有问题了。按正常的思维，namesrv配置难道不是应该跟着 producer 吗。难道 rocketMQ 在同一个 JVM 中只允许连一个 namesrv?如果是这样，那是配置的覆盖还是单例来实现呢?</p><p>于是继续看源码。</p><p>异常是在这行抛出</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200617101631909.png" alt="image-20200617101631909"></p><p>抛出的原因是下面这行判断不通过。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">topicPublishInfo !&#x3D; null &amp;&amp; topicPublishInfo.ok()</span><br></pre></td></tr></table></figure><p>继续追踪下去，可以看到是这个 Map 中 topic 对应的 topic 路由信息不存在。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200617102300534.png" alt="image-20200617102300534"></p><p>那么这个Map又是什么时候写入信息的呢。其实是在下图这个地方。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200617102825025.png" alt="image-20200617102825025"></p><p>上面那个写入路由信息的方法又是在这个地方调用的。注意，这个方法是 MQClientInstance 里的，不知道大家看到 Instance 这个类名结尾有没有啥感觉。我是看到这个基本就往单例或者跟某个 key 绑定这方面去想。那么继续看 MQClientInstance 是怎么维护的。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200617103020755.png" alt="image-20200617103020755"></p><p>MQClientInstance 存储和生成的地方如下面两张图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200617103404096.png" alt="image-20200617103404096"></p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200617103518874.png" alt="image-20200617103518874"></p><p>可以看到是在这个地方做了个绑定，那么继续往下看。生成了 MQClientInstance 并以下图里的代码的结果为 key 放入</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200617103630933.png" alt="image-20200617103630933"></p><p>那真相就出现了，<code>同一个 JVM 中如果配置的时候不配 unitName，那么不管配多少个 producer 都只会有一个 MQClientInstance,相当于 config 也就生效一份</code> 😁。基本就看哪个 producer先初始化了。后面来的只能拿之前的那个 MQClientInstance 来获取路由信息。</p><p>再回到之前最开始的问题，业务方后面配置的 producer 是用 xml 配置的 bean，而调度中心的 producer 他们使用时在重写了 afterPropertiesSet 方法里初始化。理论上肯定是 spring 自身维护的 bean 初始化在 afterPropertiesSet 之前。然而xml里配置的value恰好又是错误的，就导致了 No route info for this topic 出现了。将 namesrv 的引用改为正确的，问题解决。</p><p>后续在 vdianmq 里可以以 warn log 的形式提醒下业务方这方面使用时需要注意一下。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;昨天一位业务的同学在我审批MQ通过后，在业务代码里加了个 producer，结果启动项目时集成的调度中心的二方包里的 producer 在通
      
    
    </summary>
    
    
      <category term="中间件" scheme="http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="RocketMQ" scheme="http://xingcici.github.io/tags/RocketMQ/"/>
    
  </entry>
  
  <entry>
    <title>Zookeepr在分布式协调中的使用</title>
    <link href="http://xingcici.github.io/2020/06/16/use-of-zookeepr-in-distributed-coordination.html"/>
    <id>http://xingcici.github.io/2020/06/16/use-of-zookeepr-in-distributed-coordination.html</id>
    <published>2020-06-16T03:00:46.686Z</published>
    <updated>2020-06-24T07:11:58.541Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="分布式" scheme="http://xingcici.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
      <category term="Zookeeper" scheme="http://xingcici.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Sofa-bolt源码阅读</title>
    <link href="http://xingcici.github.io/2020/06/16/sofabolt-source-reading.html"/>
    <id>http://xingcici.github.io/2020/06/16/sofabolt-source-reading.html</id>
    <published>2020-06-16T02:57:12.287Z</published>
    <updated>2020-06-24T06:59:04.233Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>​        SOFABolt 是蚂蚁金融服务集团开发的一套基于 Netty 实现的网络通信框架。</p><ul><li><p>为了让 Java 程序员能将更多的精力放在基于网络通信的业务逻辑实现上，而不是过多的纠结于网络底层 NIO 的实现以及处理难以调试的网络问题，Netty 应运而生。</p></li><li><p>为了让中间件开发者能将更多的精力放在产品功能特性实现上，而不是重复地一遍遍制造通信框架的轮子，SOFABolt 应运而生。</p></li></ul><p>  Bolt 名字取自迪士尼动画 - 闪电狗，是一个基于 Netty 最佳实践的轻量、易用、高性能、易扩展的通信框架。 这些年我们在微服务与消息中间件在网络通信上解决过很多问题，积累了很多经验，并持续的进行着优化和完善，我们希望能把总结出的解决方案沉淀到 SOFABolt 这个基础组件里，让更多的使用网络通信的场景能够统一受益。 目前该产品已经运用在了蚂蚁中间件的微服务 (SOFARPC)、消息中心、分布式事务、分布式开关、以及配置中心等众多产品上。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;​        SOFABolt 是蚂蚁金融服务集团开发的一套基于 Netty 实现的网络通信框架。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;为了让 Java 程序员能将更多的精力放在基于网络通信的业务逻辑实现上，而不是过多的纠结于网络底层 NIO 的实
      
    
    </summary>
    
    
      <category term="源码" scheme="http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="网络" scheme="http://xingcici.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Netty" scheme="http://xingcici.github.io/tags/Netty/"/>
    
  </entry>
  
  <entry>
    <title>Raft算法的简单实现</title>
    <link href="http://xingcici.github.io/2020/06/15/simple-implementation-of-raft-algorithm.html"/>
    <id>http://xingcici.github.io/2020/06/15/simple-implementation-of-raft-algorithm.html</id>
    <published>2020-06-15T12:02:55.454Z</published>
    <updated>2020-06-24T06:59:04.235Z</updated>
    
    <content type="html"><![CDATA[<p>敬请期待</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;敬请期待&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="分布式" scheme="http://xingcici.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
      <category term="算法" scheme="http://xingcici.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>调度中心任务分片方案设计和实现</title>
    <link href="http://xingcici.github.io/2020/06/12/design-and-implementation-of-task-sharding-scheme-in-dispatch-center.html"/>
    <id>http://xingcici.github.io/2020/06/12/design-and-implementation-of-task-sharding-scheme-in-dispatch-center.html</id>
    <published>2020-06-12T06:26:50.069Z</published>
    <updated>2020-06-24T06:59:04.262Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>微店任务调度中心（TOC）每天承担千万甚至亿量级的任务调度，不单单是循环的传统任务调度，还承担了类似于延时消息的一次性调度。但是目前设计上仍然比较原始，是TOC抢分布式锁后通过线程不断扫表读取需要调度的job，再通过 dubbo 调度job，形成了一人工作，他人围观的场景。虽然能满足目前的需求，但是已经越来越力不从心，在一些极端的情况下，比如某个应用注册任务激增的情况下，就会出现大量的调度延迟。于是以能无限水平扩容为目标的调度方案的改造势在必行。</p><h3 id="老架构设计"><a href="#老架构设计" class="headerlink" title="老架构设计"></a>老架构设计</h3><p>服务通过抢占zk分布式锁来获得执行权限，从数据库中扫表获得需要执行的任务。因为我们有256张分表，我们也限制成从每张分表里读取100条记录，每次最大总数也就是25600。</p><p>执行时通过 dubbo rpc 回调注册方来执行任务。</p><p>注册方执行完后会回调调度中心更新任务状态。</p><h3 id="调研"><a href="#调研" class="headerlink" title="调研"></a>调研</h3><p>调研时发现，目前开源的比较流行的调度中心主要有两种方案。一种是中心化的，例如xxl-job。另外一种是去中心化的，例如 elastic-job。</p><p>由于公司内部zookeeper的使用已经比较成熟了，而且对高可用的要求比较高。于是着重研究了es-job。</p><p>es-job实现分片的核心在于利用zk作为协调者来进行分片。</p><p>关键流程如下：</p><p>1、每一个服务启动时，向zk的worker节点注册，同时参与leader的选举。</p><p>2、leader选举成功后，在需要进行分片的任务执行时，获取worker节点的数量，对任务进行切分，分配至worker节点下。</p><p>3、worker节点执行任务时从cache读取需要执行的任务进行执行，同时更新任务执行的状态。</p><p>关键的流程是比较简单的，主要就是利用了zk。</p><h3 id="新架构设计"><a href="#新架构设计" class="headerlink" title="新架构设计"></a>新架构设计</h3><p>我们公司的调度产品有个特点，底层数据库就是分表的，固定的256张表，那么存在天然的分片优势。</p><p>于是，我们在设计时也就比较简单，将256张表分给注册在zk worker节点下的worker就行。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>1、调度中心起来后，向zk 注册 worker临时节点，同时监听链接状态和节点 node cache 的变化。</p><p>2、每个实例都参与 leader 选举。成为leader 的实例 对 worker 根节点添加监听器，监听 worker 变化事件。</p><p>3、leader 根据 worker 数量，对 256 张表进行分配，分配结果写入到 node cache。</p><p>4、worker 监听到 node cache 发生变化，获取最新数据， 写入本地的缓存。</p><p>5、当 worker 与 zk 之间连接发生波动时，清空本地缓存。</p><p>6、调度扫描线程根据本地缓存里的表，进行扫表执行。</p><h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p>老架构CPU监控</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200612143209085.png" alt="image-20200612143209085"></p><p>新架构CPU监控</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/image-20200612143227022.png" alt="image-20200612143227022"></p><p>可以看到，一人干活，他人围观的场面一去不复返了。大家一起干活，很和谐。</p><h3 id="探索"><a href="#探索" class="headerlink" title="探索"></a>探索</h3><h3 id="etc"><a href="#etc" class="headerlink" title="etc"></a>etc</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;微店任务调度中心（TOC）每天承担千万甚至亿量级的任务调度，不单单是循环的传统任务调度，还承担了类似于延时消息的一次性调度。但是目前设计上仍
      
    
    </summary>
    
    
      <category term="架构设计" scheme="http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
  </entry>
  
  <entry>
    <title>dubbo调用在2.7.5版本以下的性能问题的解决</title>
    <link href="http://xingcici.github.io/2020/06/11/dubbo-call-to-solve-performance-problems-below-version-275.html"/>
    <id>http://xingcici.github.io/2020/06/11/dubbo-call-to-solve-performance-problems-below-version-275.html</id>
    <published>2020-06-11T02:26:30.501Z</published>
    <updated>2020-06-24T06:59:04.285Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>toc 的监控在日常环境测试时，发现日常环境在大量JOB需要调度的情况下，发现三台机器调度的瓶颈大概为8k每分钟，也就是一台机器每秒处理44个，平均每个JOB耗时22毫秒。这还是在已经分片的情况下。</p><p>如果只是 dubbo 调用就要这么久肯定不正常。于是进行排查。</p><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>理论上最耗时的点应该是在远程调用的过程，也就是网络延迟。在日常环境用arthas抓取方法耗时，发现耗时的过程反而是在</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/652BE8F3-6A34-467F-BCE0-5362B40E1E93.png" alt="652BE8F3-6A34-467F-BCE0-5362B40E1E93"></p><p>这个方法上。</p><p>用 arthas 抓的图如图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/9C1437EC-09DE-4039-9800-C68D1591701D.png" alt="9C1437EC-09DE-4039-9800-C68D1591701D"></p><p>可以看到 config.get 这个方法远大于或者接近实际调用过程。理论上对这个点进行优化可以至少提升一倍的性能。</p><h3 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h3><p>继续分析 get 方法，其内部耗时主要还是在 checkAndUpdateSubConfigs(); 这个方法。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/A0BCC225-A330-45BB-AC2F-4C73EB38AABB.png" alt="A0BCC225-A330-45BB-AC2F-4C73EB38AABB"></p><p>这个方法主要是获取刷新各种配置。而且由于我们没有用新版的dubbo配置中心，导致每调一次GET会打一次warn日志<img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/42EDB6B8-5D43-4E59-BD73-D5F22C7CB69B.png" alt="42EDB6B8-5D43-4E59-BD73-D5F22C7CB69B"></p><p>于是考虑要不在get 做个缓存来解决。但是在解决之前也查一下有没有其他人碰到这个问题，结果还真有。他们解决的办法是升级dubbo 版本。apache 2.7.5 版本</p><p>就解决了这个性能问题，将 checkAndUpdateSubConfigs 丢到 init 方法里，只在 init 的时候调用一次。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/1FFB477A-6B66-4741-8918-854868F7642B.png" alt="1FFB477A-6B66-4741-8918-854868F7642B"></p><p>不过由于升级 dubbo 核心版本没那么容易，于是暂时就在 get 外层做了个缓存来解决。</p><p>下图是修改后的耗时，可以看到耗时基本只剩调用这块。具体的性能提升多少等待后续的测试。</p><p><img src="https://cdn.jsdelivr.net/gh/xingcici/oss@master/uPic/8015F365-6DDC-4F48-BAB3-60BAF289208A.png" alt="8015F365-6DDC-4F48-BAB3-60BAF289208A"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;toc 的监控在日常环境测试时，发现日常环境在大量JOB需要调度的情况下，发现三台机器调度的瓶颈大概为8k每分钟，也就是一台机器每秒处理44
      
    
    </summary>
    
    
      <category term="日常问题" scheme="http://xingcici.github.io/categories/%E6%97%A5%E5%B8%B8%E9%97%AE%E9%A2%98/"/>
    
    
      <category term="dubbo" scheme="http://xingcici.github.io/tags/dubbo/"/>
    
  </entry>
  
</feed>
