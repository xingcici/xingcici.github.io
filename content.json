{"meta":{"title":"一方天地","subtitle":"","description":"记录生活学习","author":"行词","url":"http://xingcici.github.io","root":"/"},"pages":[{"title":"标签","date":"2020-06-24T07:37:13.614Z","updated":"2020-06-24T07:37:13.614Z","comments":false,"path":"tags/index.html","permalink":"http://xingcici.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-06-24T07:43:54.601Z","updated":"2020-06-24T07:43:54.601Z","comments":false,"path":"categories/index.html","permalink":"http://xingcici.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"rocketmq第一次拉取消息获取位点失败导致位点回退的问题","slug":"rocketmq第一次拉取消息获取位点失败导致位点回退的问题","date":"2021-03-03T10:16:13.134Z","updated":"2021-03-03T10:20:29.255Z","comments":true,"path":"2021/03/03/the-problem-that-rocketmq-failed-to-pull-message-for-first-time-obtain-site-caused-fall-back.html","link":"","permalink":"http://xingcici.github.io/2021/03/03/the-problem-that-rocketmq-failed-to-pull-message-for-first-time-obtain-site-caused-fall-back.html","excerpt":"","text":"昨天业务同学反馈说某个 topic 的某个 Queue 发生了消息堆积，看了下其他 Queue 都是正常的，就这一个 Queue 发生了堆积，而且位点是三天前。 1234567891011121314151617181920212223client日志2021-03-02 15:15:12 WARN PullMessageService - invokeSync: wait response timeout exception, the channel[brokerip:10911]2021-03-02 15:15:12 WARN NettyClientWorkerThread_2 - receive response, but not matched any request, brokerip:109112021-03-02 15:15:12 WARN NettyClientWorkerThread_2 - RemotingCommand [code&#x3D;0, language&#x3D;JAVA, version&#x3D;317, opaque&#x3D;6774, flag(B)&#x3D;1, remark&#x3D;null, extFields&#x3D;&#123;offset&#x3D;5757922&#125;, serializeTypeCurrentRPC&#x3D;JSON]2021-03-02 15:15:12 WARN PullMessageService - fetchConsumeOffsetFromBroker exception, MessageQueue [topic&#x3D;tp_inner_order_operation, brokerName&#x3D;broker-06, queueId&#x3D;10]2021-03-02 15:15:12 WARN NettyClientPublicExecutor_3 - the pull request offset illegal, PullRequest [consumerGroup&#x3D;ts_biz_operation_group, messageQueue&#x3D;MessageQueue [topic&#x3D;tp_inner_order_operation, brokerName&#x3D;broker-06, queueId&#x3D;10], nextOffset&#x3D;-1] PullResult [pullStatus&#x3D;OFFSET_ILLEGAL, nextBeginOffset&#x3D;5531762, minOffset&#x3D;5531762, maxOffset&#x3D;5757977, msgFoundList&#x3D;0]2021-03-02 15:15:22 WARN PullMessageServiceScheduledThread - unlock messageQueue. group:ts_biz_operation_group, clientId:clientip@30487, mq:MessageQueue [topic&#x3D;tp_inner_order_operation, brokerName&#x3D;broker-06, queueId&#x3D;10]2021-03-02 15:15:22 WARN PullMessageServiceScheduledThread - fix the pull request offset, PullRequest [consumerGroup&#x3D;ts_biz_operation_group, messageQueue&#x3D;MessageQueue [topic&#x3D;tp_inner_order_operation, brokerName&#x3D;broker-06, queueId&#x3D;10], nextOffset&#x3D;5531762]2021-03-02 15:15:45 WARN PullMessageService - the consumer message buffer is full, so do flow control, minOffset&#x3D;5531803, maxOffset&#x3D;5532809, size&#x3D;1008, pullRequest&#x3D;PullRequest [consumerGroup&#x3D;ts_biz_operation_group, messageQueue&#x3D;MessageQueue [topic&#x3D;tp_inner_order_operation, brokerName&#x3D;broker-06, queueId&#x3D;10], nextOffset&#x3D;5532810], flowControlTimes&#x3D;1 broker日志broker.log:2021-03-02 15:15:12 INFO PullMessageThread_5 - the request offset too small. group&#x3D;ts_biz_operation_group, topic&#x3D;tp_inner_order_operation, requestOffset&#x3D;-1, brokerMinOffset&#x3D;5531762, clientIp&#x3D;&#x2F;clientip:24944broker.log:2021-03-02 15:15:12 WARN PullMessageThread_5 - PULL_OFFSET_MOVED:correction offset. topic&#x3D;tp_inner_order_operation, groupId&#x3D;ts_biz_operation_group, requestOffset&#x3D;-1, newOffset&#x3D;5531762, suggestBrokerId&#x3D;0broker.log:2021-03-02 15:15:22 WARN ConsumerManageThread_15 - [NOTIFYME]update consumer offset less than store. clientHost&#x3D;clientip:24944, key&#x3D;tp_inner_order_operation@ts_biz_operation_group, queueId&#x3D;10, requestOffset&#x3D;5531762, storeOffset&#x3D;5757922store.log:2021-03-02 04:54:08 INFO StoreScheduledThread1 - Compute logical min offset: 5531762, topic: tp_inner_order_operation, queueId: 10 结合上面的日志和 rocketmq 的代码。基本可以推断出事情是这样发生的。 160 这台服务器进行发布，客户端初始化，向 broker 拉取位点（请求1）。 这次请求超时，客户端把 offset = -1 当做 PullRequest 参数发从给 broker（请求2）。 请求1 已经返回了，但是因为 rmq client 的 fastfail 机制对应的 request 已经不存在了。 broker 收到请求2，内部逻辑是当请求的 offset = -1 的时候，返回该 topic 当前还存在的文件的最小 offset 55w。 client 收到最小 offset 再拉取消息，把 broker 的保存的之前的位点 57w 覆盖了。 大概的逻辑就是这样，就是感觉这部分其实有点优化空间，不太理解 RMQ 为什么不拿之前保存在 map 里的位点返回，而是直接返回最小。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://xingcici.github.io/tags/RocketMQ/"}]},{"title":"HTTPS 加密协商过程","slug":"HTTPS 加密协商过程","date":"2021-02-01T09:03:59.926Z","updated":"2021-02-02T03:03:20.028Z","comments":true,"path":"2021/02/01/https-encryption-negotiation-process.html","link":"","permalink":"http://xingcici.github.io/2021/02/01/https-encryption-negotiation-process.html","excerpt":"","text":"HTTPS协商过程中涉及两个角色：client、server。client 大部分情况下可以认为是浏览器。这里会省略去一些加密协议之类的东西。 1、client 生成随机数 client_random，发送给 server。 2、server 生成随机数 server_random，再加上证书和签名，返回给 client，证书中包含公钥。 3、client 对证书进行 hash 生成 hash 串，然后用浏览器内置的 CA 公钥对签名进行验证，对比 hash 串和签名验证结果。 4、client 再生成一个随机数，与 client_random和 server_random 一起生成一个 shared secret(最终加密用), 再对这次生成的随机数用公钥进行加密生成（pre-master key），发送给 server。同时发送一个用 shared secret 加密的验证消息。 5、server 对 pre-master key 用私钥进行解密，获得随机数，再用 client_random、server_random 一起也算出 shared secret，对 client 发送的验证消息进行验证。再回复 client 一个也用 shared secret 加密的数据，表示自己准备好了。 6、在后续的传输过程中，还会对传输内容进行摘要，来验证内容的完整性。","categories":[{"name":"加密","slug":"加密","permalink":"http://xingcici.github.io/categories/%E5%8A%A0%E5%AF%86/"}],"tags":[{"name":"https","slug":"https","permalink":"http://xingcici.github.io/tags/https/"}]},{"title":"synchronized 和 static synchronized","slug":"synchronized 和 static synchronized","date":"2021-01-28T10:03:14.200Z","updated":"2021-01-28T10:08:00.494Z","comments":true,"path":"2021/01/28/synchronized-and-static.html","link":"","permalink":"http://xingcici.github.io/2021/01/28/synchronized-and-static.html","excerpt":"","text":"挺简单的一个问题，今天被人问起了顺手写一下。 其实 synchronized 是实例对象锁，而 static 是类对象锁。每个类类对象在一个classloader只会有一个。 下面两段简单的代码就能验证 1234567891011121314151617181920212223242526272829package lock;import com.meta.mq.common.utils.ThreadUtil;&#x2F;** * @author : haifeng.pang. * @version 0.1 : Human v0.1 2021&#x2F;1&#x2F;28 下午5:48 By haifeng.pang. * @description : *&#x2F;public class Human &#123; public synchronized void say(String name) &#123; System.out.println(&quot;say &quot; + name); ThreadUtil.quietSleep(10000); &#125; public synchronized void eat(String name) &#123; System.out.println(&quot;eat &quot; + name); &#125; public static synchronized void sayStatic(String name) &#123; System.out.println(&quot;static say&quot; + name); &#125; public static synchronized void eatStatic(String name) &#123; System.out.println(&quot;static eat&quot; + name); ThreadUtil.quietSleep(10000); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package lock;&#x2F;** * @author : haifeng.pang. * @version 0.1 : Test v0.1 2021&#x2F;1&#x2F;28 下午5:49 By haifeng.pang. * @description : *&#x2F;public class Test &#123; @org.junit.Test public void test() &#123; Human human &#x3D; new Human(); Human human1 &#x3D; new Human(); Thread threadA &#x3D; new Thread(new Runnable() &#123; @Override public void run() &#123; human.say(&quot;im threadA&quot;); &#125; &#125;); Thread threadB &#x3D; new Thread(new Runnable() &#123; @Override public void run() &#123; human.eat(&quot;im threadB&quot;); &#125; &#125;); threadA.start(); threadB.start(); try &#123; threadA.join(); threadB.join(); &#125;catch (Exception e) &#123; &#x2F;&#x2F; &#125; &#125;&#125;","categories":[{"name":"源码","slug":"源码","permalink":"http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://xingcici.github.io/tags/jdk/"}]},{"title":"去中心化消息队列 Meta MQ","slug":"去中心化消息队列 Meta MQ","date":"2021-01-27T12:25:24.885Z","updated":"2021-01-27T12:32:14.453Z","comments":true,"path":"2021/01/27/decentralized-message-queue-meta-mq.html","link":"","permalink":"http://xingcici.github.io/2021/01/27/decentralized-message-queue-meta-mq.html","excerpt":"","text":"虽然网上很多人说不要反复造轮子，但是就我个人的想法是造自己没造过的轮子是有利于技术的进步的。 最近时间稍微宽裕了一点，开始写一个去中心化的消息队列，暂时命名为 Meta MQ。 通过写这个消息队列，不断来深入理解业界目前消息队列的设计和原理，以及分布式协议，网络，存储方面的知识。 目前完成了最初步的功能，后面会不断得添砖加瓦~ Meta MQ 地址","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xingcici.github.io/tags/MQ/"}]},{"title":"MR产出数据体积变大问题","slug":"MR产出数据体积变大问题","date":"2021-01-27T08:45:19.027Z","updated":"2021-01-27T09:04:17.617Z","comments":true,"path":"2021/01/27/mr-output-data-volume-becomes-larger.html","link":"","permalink":"http://xingcici.github.io/2021/01/27/mr-output-data-volume-becomes-larger.html","excerpt":"","text":"一、现象前段时间在做数据同步任务的切换，在切换到MR后，总共36个任务，10个变小，16个不变，10个变大，而且变大的体积基本上都是翻倍。另外公司的存储空间当时较为紧张，所以需要解决这个体积增大的问题。 下图是用 parquet-tools 解析文件获得到，可以看到，左侧 outscan 比 右侧 mr 在好几个column上压缩率都高不少 ods_trd_buyer_order_app_order_desc_info_d ods_itm_wd_inv_app_item_sku_d 则更加明显 二、研究1、outscan（之前的同步方式，下面不再注释）和mr同步的区别。 outscan mr 直接 map hfile，没有reduce过程 分别map text 增量和 parquet 全量，再进行reduce 输出类为 AvroParquetOutputFormat 输出类为 ParquetOutputFormat 2、尝试调整 parquet 参数先放一张 parquet 文件格式图 可以看到 parquet 文件我们最需要关心的部分为 RowGroup、Column、 Page 。 当我们写 parquet 文件时，先会构造 RowGroup，然后根据不同的列构造不同的 Column，再分别在 Column 中构造 Page，然后向 Page 中写入数据，在每次写入时，会由 accountForValueWritten() 判断是否需要 writePage ，如果需要的话会把当前 Column 中的 数据输出到一个 Collect，snappy 压缩也是在这个过程。当整个RowGroup到达整个 parquet.block.size 大小时，会把整个 RowGroup 输出到 hdfs。 尝试调整的参数 调整到的值 对体积大小的影响 parquet.block.size 1G 基本无影响 parquet.page.size 1-128M 基本无影响 parquet.dictionary.page.size 1-128M 当只读 text 时，对体积约有20%的减小，在线上跑时，对体积基本无影响 io.compression.codec.snappy.buffersize 1M 基本无影响 io.file.buffer.size 1M 基本无影响 输出类改为 AvroParquet 基本无影响 升级 parquet 版本 基本无影响 3、查看 outscan 和 mr 的日志区别a) outscan12345678910111213141516171819202122232425262728293031323334Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPYDec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is onDec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is offDec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0Dec 11, 2020 4:08:22 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytesDec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134,279,407 &gt; 134,217,728: flushing 1,035,273 records to disk.Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134,097,175Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,122,796B for [auto_id] INT64: 1,035,273 values, 7,512,463B raw, 4,122,414B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 94,035 entries, 752,280B raw, 94,035B comp&#125;Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,925,458B for [id] INT64: 1,035,273 values, 8,282,248B raw, 5,925,075B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE]Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,213,757B for [item_id] INT64: 1,035,273 values, 7,507,266B raw, 4,213,375B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 83,192 entries, 665,536B raw, 83,192B comp&#125;Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,449,023B for [price] BINARY: 1,035,273 values, 1,548,410B raw, 1,448,622B comp, 9 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 13,877 entries, 135,544B raw, 13,877B comp&#125;Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 305,529B for [quantity] INT64: 1,035,273 values, 601,051B raw, 305,153B comp, 8 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1,455 entries, 11,640B raw, 1,455B comp&#125;Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,585,004B for [total_price] BINARY: 1,035,273 values, 1,726,198B raw, 1,584,599B comp, 9 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 33,741 entries, 334,219B raw, 33,741B comp&#125;Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 25,257B for [cps_fee] BINARY: 1,035,273 values, 50,010B raw, 24,968B comp, 8 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 314 entries, 2,627B raw, 314B comp&#125;Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,715,269B for [item_sku_id] INT64: 1,035,273 values, 6,616,763B raw, 2,714,888B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 104,273 entries, 834,184B raw, 104,273B comp&#125;Dec 11, 2020 4:08:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,605,807B for [add_time] BINARY: 1,035,273 values, 22,843,179B raw, 6,604,199B comp, 23 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 27,693 entries, 636,939B raw, 27,693B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,098,234B for [order_id] INT64: 1,035,273 values, 7,490,983B raw, 4,097,852B comp, 8 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 76,367 entries, 610,936B raw, 76,367B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 402B for [sk] BINARY: 1,035,273 values, 94B raw, 102B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 7 entries, 392B raw, 7B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,835,688B for [item_sku_title] BINARY: 1,035,273 values, 12,055,000B raw, 5,834,969B comp, 13 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 23,553 entries, 567,428B raw, 23,553B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 34,929,921B for [item_title] BINARY: 1,035,273 values, 67,065,375B raw, 34,918,001B comp, 65 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 9,636 entries, 720,750B raw, 9,636B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 23,697,146B for [img_head] BINARY: 1,035,273 values, 62,188,305B raw, 23,689,804B comp, 61 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 11,114 entries, 692,307B raw, 11,114B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,024,817B for [extend] BINARY: 1,035,273 values, 59,829,153B raw, 15,022,915B comp, 56 pages, encodings: [BIT_PACKED, PLAIN, RLE]Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 456B for [data_ver] INT64: 1,035,273 values, 96B raw, 112B comp, 8 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1 entries, 8B raw, 1B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,496,780B for [update_time] BINARY: 1,035,273 values, 22,843,365B raw, 6,495,172B comp, 23 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 27,530 entries, 633,190B raw, 27,530B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 486,705B for [buyer_id] BINARY: 1,035,273 values, 1,757,571B raw, 486,033B comp, 14 pages, encodings: [BIT_PACKED, PLAIN, RLE, PLAIN_DICTIONARY], dic &#123; 72,340 entries, 980,153B raw, 72,340B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,248,275B for [seller_id] INT32: 1,035,273 values, 1,440,252B raw, 1,248,119B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 112,471 entries, 449,884B raw, 112,471B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 164,710B for [f_seller_id] INT32: 1,035,273 values, 251,370B raw, 164,554B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 12,392 entries, 49,568B raw, 12,392B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 149,608B for [status] INT32: 1,035,273 values, 158,031B raw, 149,452B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 3 entries, 12B raw, 3B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 456B for [rate_fund] INT64: 1,035,273 values, 96B raw, 112B comp, 8 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1 entries, 8B raw, 1B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 169,390B for [version_id] INT32: 1,035,273 values, 231,460B raw, 169,234B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 13 entries, 52B raw, 13B comp&#125;Dec 11, 2020 4:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67,464B for [refund_status] INT32: 1,035,273 values, 89,951B raw, 67,308B comp, 4 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 4 entries, 16B raw, 4B comp&#125; b) mr12345678910111213141516171819202122232425262728293031323334Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPYDec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is onDec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is offDec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0Dec 11, 2020 12:09:45 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytesDec 11, 2020 12:10:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134,596,338 &gt; 134,217,728: flushing 679,290 records to disk.Dec 11, 2020 12:10:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 135,665,494Dec 11, 2020 12:10:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 84,603B for [auto_id] INT64: 679,290 values, 84,384B raw, 84,323B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 22,331 entries, 178,648B raw, 22,331B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,121,794B for [id] INT64: 679,290 values, 5,434,368B raw, 5,121,507B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN]Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,114,386B for [item_id] INT64: 679,290 values, 5,434,368B raw, 4,114,099B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN]Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,140,171B for [price] BINARY: 679,290 values, 1,158,473B raw, 1,139,909B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 12,680 entries, 123,547B raw, 12,680B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 281,422B for [quantity] INT64: 679,290 values, 643,817B raw, 281,140B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1,213 entries, 9,704B raw, 1,213B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,225,975B for [total_price] BINARY: 679,290 values, 1,243,295B raw, 1,225,712B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 27,429 entries, 269,122B raw, 27,429B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,977B for [cps_fee] BINARY: 679,290 values, 2,390B raw, 1,760B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 165 entries, 1,357B raw, 165B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,572,263B for [item_sku_id] INT64: 679,290 values, 4,662,155B raw, 2,571,977B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 70,731 entries, 565,848B raw, 70,731B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,933,270B for [add_time] BINARY: 679,290 values, 15,623,790B raw, 6,932,221B comp, 15 pages, encodings: [BIT_PACKED, RLE, PLAIN]Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,037,909B for [order_id] INT64: 679,290 values, 5,434,368B raw, 5,037,622B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN]Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 332B for [sk] BINARY: 679,290 values, 117B raw, 123B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 9 entries, 548B raw, 9B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 5,675,428B for [item_sku_title] BINARY: 679,290 values, 7,868,590B raw, 5,674,984B comp, 9 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 30,819 entries, 729,235B raw, 30,819B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 34,917,594B for [item_title] BINARY: 679,290 values, 41,284,243B raw, 34,910,519B comp, 41 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 15,403 entries, 1,011,183B raw, 15,403B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 27,409,350B for [img_head] BINARY: 679,290 values, 41,769,032B raw, 27,405,580B comp, 41 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 15,612 entries, 1,014,148B raw, 15,612B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 16,932,320B for [extend] BINARY: 679,290 values, 42,638,909B raw, 16,930,559B comp, 42 pages, encodings: [BIT_PACKED, RLE, PLAIN, PLAIN_DICTIONARY], dic &#123; 17,372 entries, 1,008,909B raw, 17,372B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 342B for [data_ver] INT64: 679,290 values, 72B raw, 84B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1 entries, 8B raw, 1B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,800,709B for [update_time] BINARY: 679,290 values, 15,623,790B raw, 6,799,660B comp, 15 pages, encodings: [BIT_PACKED, RLE, PLAIN]Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,194,701B for [buyer_id] BINARY: 679,290 values, 9,200,418B raw, 6,194,344B comp, 9 pages, encodings: [BIT_PACKED, RLE, PLAIN]Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,464,540B for [seller_id] INT32: 679,290 values, 1,464,273B raw, 1,464,423B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 131,083 entries, 524,332B raw, 131,083B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 295,854B for [f_seller_id] INT32: 679,290 values, 550,175B raw, 295,737B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 16,979 entries, 67,916B raw, 16,979B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 152,287B for [status] INT32: 679,290 values, 161,460B raw, 152,170B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 3 entries, 12B raw, 3B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 342B for [rate_fund] INT64: 679,290 values, 72B raw, 84B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 1 entries, 8B raw, 1B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 226,851B for [version_id] INT32: 679,290 values, 309,556B raw, 226,734B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 16 entries, 64B raw, 16B comp&#125;Dec 11, 2020 12:10:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65,837B for [refund_status] INT32: 679,290 values, 101,856B raw, 65,720B comp, 3 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic &#123; 4 entries, 16B raw, 4B comp&#125; 可以看到 parquet.block.size 、page.size 、 dictionary.page.size 这些参数都一样的情况下，在一个 rowgroup 中 mr 能写入的数据少于 outscan 能写入的数据，这就是因为压缩率不同导致的。 3、查看源码a) snappy1org.apache.parquet.hadoop.CodecFactory.BytesCompressor#compress public BytesInput compress(BytesInput bytes) throws IOException &#123; final BytesInput compressedBytes; if (codec &#x3D;&#x3D; null) &#123; compressedBytes &#x3D; bytes; &#125; else &#123; compressedOutBuffer.reset(); if (compressor !&#x3D; null) &#123; &#x2F;&#x2F; null compressor for non-native gzip compressor.reset(); &#125; CompressionOutputStream cos &#x3D; codec.createOutputStream(compressedOutBuffer, compressor); bytes.writeAllTo(cos); cos.finish(); cos.close(); compressedBytes &#x3D; BytesInput.from(compressedOutBuffer); &#125; return compressedBytes; &#125; org.apache.parquet.hadoop.codec.SnappyCompressor#compress @Override public synchronized int compress(byte[] buffer, int off, int len) throws IOException &#123; SnappyUtil.validateBuffer(buffer, off, len); if (needsInput()) &#123; &#x2F;&#x2F; No buffered output bytes and no input to consume, need more input return 0; &#125; if (!outputBuffer.hasRemaining()) &#123; &#x2F;&#x2F; There is uncompressed input, compress it now int maxOutputSize &#x3D; Snappy.maxCompressedLength(inputBuffer.position()); if (maxOutputSize &gt; outputBuffer.capacity()) &#123; outputBuffer &#x3D; ByteBuffer.allocateDirect(maxOutputSize); &#125; &#x2F;&#x2F; Reset the previous outputBuffer outputBuffer.clear(); inputBuffer.limit(inputBuffer.position()); inputBuffer.position(0); int size &#x3D; Snappy.compress(inputBuffer, outputBuffer); outputBuffer.limit(size); inputBuffer.limit(0); inputBuffer.rewind(); &#125; &#x2F;&#x2F; Return compressed output up to &#39;len&#39; int numBytes &#x3D; Math.min(len, outputBuffer.remaining()); outputBuffer.get(buffer, off, numBytes); bytesWritten +&#x3D; numBytes; return numBytes; &#125; org.xerial.snappy.Snappy#compress(java.nio.ByteBuffer, java.nio.ByteBuffer) public static int compress(ByteBuffer uncompressed, ByteBuffer compressed) throws IOException &#123; if (!uncompressed.isDirect()) throw new SnappyError(SnappyErrorCode.NOT_A_DIRECT_BUFFER, &quot;input is not a direct buffer&quot;); if (!compressed.isDirect()) throw new SnappyError(SnappyErrorCode.NOT_A_DIRECT_BUFFER, &quot;destination is not a direct buffer&quot;); &#x2F;&#x2F; input: uncompressed[pos(), limit()) &#x2F;&#x2F; output: compressed int uPos &#x3D; uncompressed.position(); int uLen &#x3D; uncompressed.remaining(); int compressedSize &#x3D; ((SnappyNativeAPI) impl).rawCompress(uncompressed, uPos, uLen, compressed, compressed.position()); &#x2F;&#x2F; pos limit &#x2F;&#x2F; [ ......BBBBBBB.........] compressed.limit(compressed.position() + compressedSize); return compressedSize; &#125; 上面是 snappy 压缩的过程，可以看到 snappy 对输入的流除了不能超过 Integer.MAXVALUE，其他没有做什么限制。也就是输入什么，它就按它的逻辑去压缩什么。parquet 获取到压缩后的流写入内存 page 中。 因此推测问题应该不在 snappy 这边。 b) parquetparquet 源码我就不复制过来了，有点多。 parquet 也只是获取到 reduce 的输出流，对数据根据类型和内容进行不同的编码，然后形成它的数据结构再用 snappy 进行压缩，再输出到文件。 需要注意的是 parquet.dictionary.page.size 这个参数会对编码造成一定影响，当 dictionary 里的值大于配置的值，编码会退化。 c) mapreduce重点怀疑是在 reduce 后或者过程中，输出数据之间较为稀疏，导致压缩效率不理想。但是这块不是非常熟悉，需要找方法验证。 三、结果已经找到原因，确实是 reduce 后数据较为稀疏导致压缩率不高。 前天在排查的时候，发现 outscan 出来的数据某些 ID 都是连续的，而后面的 title 数据会有基本相同的数据排在一起。怀疑某些表的数据有特殊性，在 ID 连续时，其他列的数据基本相同。后续也证实了这个猜想。 当某些表的 xxx_id 连续分布在同一个 reduce 中时，会对压缩率有极大的提高，例如 | item_sku_id| title || 11231 | 黑色羊毛衫XL|| 11232 | 黑色羊毛衫XXL| | 11233 | 遥控汽车A | | 11234 | 遥控汽车B | 默认的 partition 会将 11231，11233发送到 reduce A , 11232 ，11234 发送到 reduce B,最后产生两个内容基本没有重复的文件，会导致数据不连续 压缩率不理想。 而 outscan 没有 reduce 的过程，直接 map 完有序输出，不会将连续的 id 切分到各个 reduce。 解决办法 1public int getPartition(Text text, Text value, int numPartitions) &#123; long key &#x3D; Long.parseLong((text.toString().split(&quot;_&quot;))[0]); return (int) (key&#x2F;100%numPartitions); &#125; 增加了一个 partition，专门给这些数据上有连续性的表用，将连续的 ID 发送到同一个 reduce 输出。sub最后两位是观察我们DB里的数据的连续性得出的经验值。 四、解决问题之外的研究最后研究下 parquet 的编码对 parquet 本身压缩率有多大的影响。 开启 SNAPPY 压缩，自定义分区方式重跑 ods_itm_wd_inv_app_item_sku_d 任务结果体积大小及编码结果 关闭SNAPPY压缩后，自定义分区方式重新跑 ods_itm_wd_inv_app_item_sku_d 任务结果体积大小及编码结果。 关闭SNAPPY压缩后，默认分区方式重新跑 ods_itm_wd_inv_app_item_sku_d 任务结果体积大小及编码结果。 根据上面的数据可以得出，在数据密集的情况下，parquet 本身对数据的压缩率就会高一些，在数据不密集的情况下 5.1T ，数据密集的情况下 3.7T。 下面两张图可以看到，有序和无序对都是字符的 title 列的编码方式影响不大，都用PLAIN编码。","categories":[{"name":"数据智能","slug":"数据智能","permalink":"http://xingcici.github.io/categories/%E6%95%B0%E6%8D%AE%E6%99%BA%E8%83%BD/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://xingcici.github.io/tags/hadoop/"}]},{"title":"Skywalking 6.6.0对arthas的兼容性问题","slug":"Skywalking 6.6.0对arthas的兼容性问题","date":"2020-09-10T09:08:08.549Z","updated":"2020-09-11T03:29:25.141Z","comments":true,"path":"2020/09/10/skywalking-660-compatibility-issues-with-arthas.html","link":"","permalink":"http://xingcici.github.io/2020/09/10/skywalking-660-compatibility-issues-with-arthas.html","excerpt":"","text":"背景目前在参与公司全链路压测项目，在部署到日常环境服务进行测试时，用 arthas 进行追踪排查问题发现无法追踪被 agent 增强的类。于是尝试解决这个问题。 报错如下 12345678910111213141516171819202122232425262728&#x2F;&#x2F;arthas trace 时命令行报错[arthas@24580]$ trace com.mysql.jdbc.PreparedStatement executeAffect(class count: 1 , method count: 5) cost in 480 ms, listenerId: 8Enhance error! exception: java.lang.UnsupportedOperationException: class redefinition failed: attempted to change superclass or interfaceserror happens when enhancing class: class redefinition failed: attempted to change superclass or interfaces, check arthas log: &#x2F;home&#x2F;www&#x2F;logs&#x2F;arthas&#x2F;arthas.log&#x2F;&#x2F;arthas 日志报错2020-09-10 17:05:24 [arthas-command-execute] INFO c.t.arthas.core.advisor.Enhancer -enhance matched classes: [class com.mysql.jdbc.PreparedStatement, class com.mysql.jdbc.JDBC4PreparedStatement, class com.mysql.jdbc.CallableStatement, class com.mysql.jdbc.ServerPreparedStatement]2020-09-10 17:05:24 [arthas-command-execute] ERROR c.t.arthas.core.advisor.Enhancer -Enhancer error, matchingClasses: [class com.mysql.jdbc.PreparedStatement, class com.mysql.jdbc.JDBC4PreparedStatement, class com.mysql.jdbc.CallableStatement, class com.mysql.jdbc.ServerPreparedStatement]java.lang.UnsupportedOperationException: class redefinition failed: attempted to change superclass or interfaces at sun.instrument.InstrumentationImpl.retransformClasses0(Native Method) at sun.instrument.InstrumentationImpl.retransformClasses(InstrumentationImpl.java:144) at com.taobao.arthas.core.advisor.Enhancer.enhance(Enhancer.java:368) at com.taobao.arthas.core.command.monitor200.EnhancerCommand.enhance(EnhancerCommand.java:149) at com.taobao.arthas.core.command.monitor200.EnhancerCommand.process(EnhancerCommand.java:96) at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl.process(AnnotatedCommandImpl.java:82) at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl.access$100(AnnotatedCommandImpl.java:18) at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl$ProcessHandler.handle(AnnotatedCommandImpl.java:111) at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl$ProcessHandler.handle(AnnotatedCommandImpl.java:108) at com.taobao.arthas.core.shell.system.impl.ProcessImpl$CommandProcessTask.run(ProcessImpl.java:385) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 解决思路首先 skywalking 和 arthas 都是目前比较受欢迎的开源项目，我们应该不是第一个碰到这个问题的人。Github 一搜，果然不少人都碰到了这个问题（部分资料见参考资料）。 其中 arthas 维护团队的一位成员已经对 skywalking 提出了 pr，并已经合并到 matser，但是我们是 6.6.0，master 已经是 8.x。只能参考下进行改造。 基本的改造点为： 开源代码中的方案包括了 文件 和 内存缓存两种方式，我们不需要文件这种比较重量级的方式，于是去掉了，而且将内存的方式改为了LRU。 6.6.0 加载自定义 CacheableTransformerDecorator 的方式和 8.x 不太一样，主要是 bytebuddy 版本差异造成的。6.6.0 加载的地方为 agentBuilder.installOn(x,x) 。 这样改造后基本就能兼容 arthas 了。 参考资料 [skywalking 6.4.0 版本兼容性]:https://github.com/alibaba/arthas/issues/898 [support class cache for ByteBuddy]:https://github.com/apache/skywalking/pull/4858/files [How to advice a class that have been intercepted by another javaagent but not loaded]: https://github.com/raphw/byte-buddy/issues/829 [Easily Create Java Agents with Byte Buddy]:https://www.infoq.com/articles/Easily-Create-Java-Agents-with-ByteBuddy/","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"链路追踪","slug":"链路追踪","permalink":"http://xingcici.github.io/tags/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/"}]},{"title":"零知识证明","slug":"零知识证明","date":"2020-07-13T10:58:34.892Z","updated":"2020-07-13T11:20:42.193Z","comments":true,"path":"2020/07/13/zero-knowledge-proof.html","link":"","permalink":"http://xingcici.github.io/2020/07/13/zero-knowledge-proof.html","excerpt":"","text":"零知识证明：一个略微严肃的科普","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://xingcici.github.io/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"}]},{"title":"FutureTask","slug":"FutureTask","date":"2020-07-13T09:44:52.627Z","updated":"2020-07-13T09:47:11.607Z","comments":true,"path":"2020/07/13/futuretask.html","link":"","permalink":"http://xingcici.github.io/2020/07/13/futuretask.html","excerpt":"","text":"我们实现 Callable 接口，在覆写的 call 方法中定义需要执行的业务逻辑； 然后把我们实现的 Callable 接口实现对象传给 FutureTask，然后 FutureTask 作为异步任务提交给线程执行； 最重要的是 FutureTask 内部维护了一个状态 state，任何操作（异步任务正常结束与否还是被取消）都是围绕着这个状态进行，并随时更新 state 任务的状态； 只能有一个线程执行异步任务，当异步任务执行结束后，此时可能正常结束，异常结束或被取消。 可以多个线程并发获取异步任务执行结果，当异步任务还未执行完，此时获取异步任务的线程将加入线程等待列表进行等待； 当异步任务线程执行结束后，此时会唤醒获取异步任务执行结果的线程，注意唤醒顺序是 “后进先出” 即后面加入的阻塞线程先被唤醒。 当我们调用 FutureTask.cancel 方法时并不能真正停止执行异步任务的线程，只是发出中断线程的信号。但是只要 cancel 方法返回 true，此时即使异步任务能正常执行完，此时我们调用 get 方法获取结果时依然会抛出 CancellationException 异常。 利用 LockSupport 来实现线程的阻塞 \\ 唤醒机制； 利用 volatile 和 UNSAFE 的 CAS 方法来实现线程共享变量的无锁化操作； 若要编写超时异常的逻辑可以参考 FutureTask 的 get(long timeout, TimeUnit unit) 的实现逻辑； 多线程获取某一成员变量结果时若需要等待时的线程等待链表的逻辑实现； 某一异步任务在某一时刻只能由单一线程执行的逻辑实现； FutureTask 中的任务状态 state 的变化处理的逻辑实现。","categories":[{"name":"源码","slug":"源码","permalink":"http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://xingcici.github.io/tags/JDK/"}]},{"title":"RockeMQ 单机版在 centos 上的部署","slug":"RockeMQ 单机版在 centos 上的部署","date":"2020-06-30T10:14:15.737Z","updated":"2020-07-01T05:34:11.679Z","comments":true,"path":"2020/06/30/rockemq-standalone-version-deployment-on-centos.html","link":"","permalink":"http://xingcici.github.io/2020/06/30/rockemq-standalone-version-deployment-on-centos.html","excerpt":"","text":"最近需要需要再研究下 RocketMQ 的文件系统的具体实现，于是重新在自己的服务器上安装了一遍，记录下过程。 到镜像站下载安装包 wget http://mirror.bit.edu.cn/apache/rocketmq/4.7.1/rocketmq-all-4.7.1-bin-release.zip 并解压缩 修改 broker.conf 文件（conf/broker.conf）添加 naemServer 地址的属性以及自动创建 Topic 的属性。 namesrvAddr = 127.0.0.1:9876 autoCreateTopicEnable = true 修改启动参数（由于 rocketMQ 对内存的消耗比较大，所以测试时修改为本机合适的大小）。主要修改 bin 目录下的 runserver.sh 和 runbroker.sh 下的 JAVA_OPT 属性。 nohup sh bin/mqnamesrv &amp; nohup sh bin/mqbroker -c /root/download/rocketmq/conf/broker.conf &amp; sh bin/mqshutdown namesrv sh bin/mqshutdown broker 注意开放端口 有几个坑要注意 开放端口 我在部署的时候碰到 store 目录下没法自动建文件夹的问题。 broker.conf 增加配置 namesrvAddr = 127.0.0.1:9876brokerIP1=47.101.33.17autoCreateTopicEnable = true","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://xingcici.github.io/tags/RocketMQ/"}]},{"title":"反射获取一个类的私有方法","slug":"反射获取一个类的私有方法","date":"2020-06-24T09:29:25.561Z","updated":"2020-06-30T03:33:21.715Z","comments":true,"path":"2020/06/24/reflection-to-get-a-class-private-method.html","link":"","permalink":"http://xingcici.github.io/2020/06/24/reflection-to-get-a-class-private-method.html","excerpt":"","text":"比较简单 Github 123456789101112131415161718192021222324public class AccessPrivateMember &#123; public static void main(String[] args) &#123; try &#123; Class c = Class.forName(\"com.example.reflect.HelloService\"); //能获取所有有访问权限的方法，包括父类中继承的 Method publicMethod = c.getMethod(\"publicHello\", String.class); Method saySomething = c.getMethod(\"publicHello\", String.class); //获取所有方法 本方法中 Method thisClassMethod = c.getDeclaredMethod(\"privateHello\", String.class); //设置权限 thisClassMethod.setAccessible(true); //不能直接转成类来执行 classloader 不同 publicMethod.invoke(c.newInstance(), \"publicMethod\"); saySomething.invoke(c.newInstance(), \"saySomething\"); thisClassMethod.invoke(c.newInstance(), \"thisClassMethod\"); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"编程","slug":"编程","permalink":"http://xingcici.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"反射","slug":"反射","permalink":"http://xingcici.github.io/tags/%E5%8F%8D%E5%B0%84/"}]},{"title":"RocketMQ 消息存储的设计与实现","slug":"RocketMQ 消息存储的设计与实现","date":"2020-06-24T09:01:25.741Z","updated":"2020-07-01T07:10:45.920Z","comments":true,"path":"2020/06/24/design-and-implementation-of-rocketmq-message-storage.html","link":"","permalink":"http://xingcici.github.io/2020/06/24/design-and-implementation-of-rocketmq-message-storage.html","excerpt":"","text":"作为一款高性能的消息中间件，RocketMQ 基于互联网的生产要求对多 Topic 场景做了诸多针对性优化。根据中间件团队提供的压测报告，在 Producer 和 Consumer 共存的情况下，相比于 Kafka，RocketMQ 的性能指标（TPS 和 RT）随着 Topic 数量的上升表现稳定。本文从消息存储的角度谈谈 RocketMQ 高性能的原因，重点包括四个方面：消息文件存储的结构设计、消息从 Broker 接收到持久化磁盘的流程、刷盘策略和内存映射优化机制。 消息文件存储结构设计消息持久化刷盘策略内存映射","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://xingcici.github.io/tags/RocketMQ/"}]},{"title":"深入理解 JAVA 反序列化漏洞","slug":"深入理解 JAVA 反序列化漏洞","date":"2020-06-24T01:47:01.053Z","updated":"2020-06-24T06:59:04.258Z","comments":true,"path":"2020/06/24/indepth-understanding-of-java-deserialization-vulnerability.html","link":"","permalink":"http://xingcici.github.io/2020/06/24/indepth-understanding-of-java-deserialization-vulnerability.html","excerpt":"","text":"惊闻 dubbo 爆出严重的反序列化漏洞 CVE-2020-1948：Apache Dubbo 远程代码执行漏洞通告 加上之前的 fastjson 不断的反序列化漏洞，于是了解了下这方面的知识。详见深入理解 JAVA 反序列化漏洞","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"tags":[]},{"title":"Linux命令学习","slug":"Linux命令学习","date":"2020-06-23T06:16:36.652Z","updated":"2020-06-24T06:59:04.203Z","comments":true,"path":"2020/06/23/linux-command-learning.html","link":"","permalink":"http://xingcici.github.io/2020/06/23/linux-command-learning.html","excerpt":"","text":"Linux命令大全","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://xingcici.github.io/tags/linux/"}]},{"title":"编程中的一些感悟","slug":"编程中的一些感悟","date":"2020-06-18T12:20:02.180Z","updated":"2020-06-24T06:59:04.236Z","comments":true,"path":"2020/06/18/some-sentiments-in-programming.html","link":"","permalink":"http://xingcici.github.io/2020/06/18/some-sentiments-in-programming.html","excerpt":"","text":"加强对象意识之前虽然有意识在强化，但是有时候还是写出简单的命令式的代码。对同一个事物的一系列操作，基本都能以该事物发起操作的形式来设计，包装一下。 lifecycle最近开始真的写中间件才意识到，对对象的生命周期的掌握非常重要，下面的这种形式就很方便 1234567891011public class Server implements LifeCycle &#123; @Override public void startup() &#123; &#125; @Override public void shutdown() &#123; &#125;&#125;","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}],"tags":[]},{"title":"Sofa-bolt的简单使用","slug":"Sofa-bolt的简单使用","date":"2020-06-18T03:48:20.319Z","updated":"2020-06-24T06:59:04.218Z","comments":true,"path":"2020/06/18/simple-use-of-sofabolt.html","link":"","permalink":"http://xingcici.github.io/2020/06/18/simple-use-of-sofabolt.html","excerpt":"","text":"代码已经放在 github","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://xingcici.github.io/tags/%E7%BD%91%E7%BB%9C/"},{"name":"Netty","slug":"Netty","permalink":"http://xingcici.github.io/tags/Netty/"}]},{"title":"由No route info for this topic引发的关于RocketMQ的问题","slug":"由No route info for this topic引发的关于RocketMQ的问题","date":"2020-06-17T01:21:44.351Z","updated":"2020-06-24T06:59:04.329Z","comments":true,"path":"2020/06/17/questions-about-rocketmq-caused-by-no-route-info-for-this-topic.html","link":"","permalink":"http://xingcici.github.io/2020/06/17/questions-about-rocketmq-caused-by-no-route-info-for-this-topic.html","excerpt":"","text":"背景昨天一位业务的同学在我审批MQ通过后，在业务代码里加了个 producer，结果启动项目时集成的调度中心的二方包里的 producer 在通过 MQ 注册 JOB 时报 No route info for this topic。 思路这个异常也算比较常见了，一般是没有创建 topic，或者连接错了 namesrv 导致。但是查看了他的代码后发现配置没有问题。然后他反馈他 DEBUG 出来 producer 里的 namesrv 是正确的，然后他自己新加的。这就很奇怪了，于是我让他提交了代码然后申请了代码权限看下原因。 解决这里先说一句，我对 BUG 这种东西，一直都相信一句话，99%你碰到觉得无比高深的 BUG，基本都是非常简单的点弄错了导致的。这次果然还是如此。 一开始我都没去看配置的问题，因为从业务方之前截图给我的配置里是没问题的。我们对配置的引用有两种方式，一种是 @key@，另外一种是 ${key}。截图给我的时候用的是 @key@，我想当然觉得这应该也是能正常拿到 value 的。 于是开始DEBUG，发现他配置的 @key@ 居然没引用到 value，直接把 @key @作为 namesrv 传进去了,如下图所示。 但是思考一下，又有问题了。按正常的思维，namesrv配置难道不是应该跟着 producer 吗。难道 rocketMQ 在同一个 JVM 中只允许连一个 namesrv?如果是这样，那是配置的覆盖还是单例来实现呢? 于是继续看源码。 异常是在这行抛出 抛出的原因是下面这行判断不通过。 1topicPublishInfo !&#x3D; null &amp;&amp; topicPublishInfo.ok() 继续追踪下去，可以看到是这个 Map 中 topic 对应的 topic 路由信息不存在。 那么这个Map又是什么时候写入信息的呢。其实是在下图这个地方。 上面那个写入路由信息的方法又是在这个地方调用的。注意，这个方法是 MQClientInstance 里的，不知道大家看到 Instance 这个类名结尾有没有啥感觉。我是看到这个基本就往单例或者跟某个 key 绑定这方面去想。那么继续看 MQClientInstance 是怎么维护的。 MQClientInstance 存储和生成的地方如下面两张图所示。 可以看到是在这个地方做了个绑定，那么继续往下看。生成了 MQClientInstance 并以下图里的代码的结果为 key 放入 那真相就出现了，同一个 JVM 中如果配置的时候不配 unitName，那么不管配多少个 producer 都只会有一个 MQClientInstance,相当于 config 也就生效一份 😁。基本就看哪个 producer先初始化了。后面来的只能拿之前的那个 MQClientInstance 来获取路由信息。 再回到之前最开始的问题，业务方后面配置的 producer 是用 xml 配置的 bean，而调度中心的 producer 他们使用时在重写了 afterPropertiesSet 方法里初始化。理论上肯定是 spring 自身维护的 bean 初始化在 afterPropertiesSet 之前。然而xml里配置的value恰好又是错误的，就导致了 No route info for this topic 出现了。将 namesrv 的引用改为正确的，问题解决。 后续在 vdianmq 里可以以 warn log 的形式提醒下业务方这方面使用时需要注意一下。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://xingcici.github.io/tags/RocketMQ/"}]},{"title":"Zookeepr在分布式协调中的使用","slug":"Zookeepr在分布式协调中的使用","date":"2020-06-16T03:00:46.686Z","updated":"2020-06-24T07:11:58.541Z","comments":true,"path":"2020/06/16/use-of-zookeepr-in-distributed-coordination.html","link":"","permalink":"http://xingcici.github.io/2020/06/16/use-of-zookeepr-in-distributed-coordination.html","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://xingcici.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://xingcici.github.io/tags/Zookeeper/"}]},{"title":"Sofa-bolt源码阅读","slug":"Sofa-bolt源码阅读","date":"2020-06-16T02:57:12.287Z","updated":"2020-06-24T06:59:04.233Z","comments":true,"path":"2020/06/16/sofabolt-source-reading.html","link":"","permalink":"http://xingcici.github.io/2020/06/16/sofabolt-source-reading.html","excerpt":"","text":"​ SOFABolt 是蚂蚁金融服务集团开发的一套基于 Netty 实现的网络通信框架。 为了让 Java 程序员能将更多的精力放在基于网络通信的业务逻辑实现上，而不是过多的纠结于网络底层 NIO 的实现以及处理难以调试的网络问题，Netty 应运而生。 为了让中间件开发者能将更多的精力放在产品功能特性实现上，而不是重复地一遍遍制造通信框架的轮子，SOFABolt 应运而生。 Bolt 名字取自迪士尼动画 - 闪电狗，是一个基于 Netty 最佳实践的轻量、易用、高性能、易扩展的通信框架。 这些年我们在微服务与消息中间件在网络通信上解决过很多问题，积累了很多经验，并持续的进行着优化和完善，我们希望能把总结出的解决方案沉淀到 SOFABolt 这个基础组件里，让更多的使用网络通信的场景能够统一受益。 目前该产品已经运用在了蚂蚁中间件的微服务 (SOFARPC)、消息中心、分布式事务、分布式开关、以及配置中心等众多产品上。","categories":[{"name":"源码","slug":"源码","permalink":"http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://xingcici.github.io/tags/%E7%BD%91%E7%BB%9C/"},{"name":"Netty","slug":"Netty","permalink":"http://xingcici.github.io/tags/Netty/"}]},{"title":"Raft算法的简单实现","slug":"Raft算法的简单实现","date":"2020-06-15T12:02:55.454Z","updated":"2020-06-24T06:59:04.235Z","comments":true,"path":"2020/06/15/simple-implementation-of-raft-algorithm.html","link":"","permalink":"http://xingcici.github.io/2020/06/15/simple-implementation-of-raft-algorithm.html","excerpt":"","text":"敬请期待","categories":[{"name":"分布式","slug":"分布式","permalink":"http://xingcici.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://xingcici.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"调度中心任务分片方案设计和实现","slug":"调度中心任务分片方案设计和实现","date":"2020-06-12T06:26:50.069Z","updated":"2020-06-24T06:59:04.262Z","comments":true,"path":"2020/06/12/design-and-implementation-of-task-sharding-scheme-in-dispatch-center.html","link":"","permalink":"http://xingcici.github.io/2020/06/12/design-and-implementation-of-task-sharding-scheme-in-dispatch-center.html","excerpt":"","text":"背景微店任务调度中心（TOC）每天承担千万甚至亿量级的任务调度，不单单是循环的传统任务调度，还承担了类似于延时消息的一次性调度。但是目前设计上仍然比较原始，是TOC抢分布式锁后通过线程不断扫表读取需要调度的job，再通过 dubbo 调度job，形成了一人工作，他人围观的场景。虽然能满足目前的需求，但是已经越来越力不从心，在一些极端的情况下，比如某个应用注册任务激增的情况下，就会出现大量的调度延迟。于是以能无限水平扩容为目标的调度方案的改造势在必行。 老架构设计服务通过抢占zk分布式锁来获得执行权限，从数据库中扫表获得需要执行的任务。因为我们有256张分表，我们也限制成从每张分表里读取100条记录，每次最大总数也就是25600。 执行时通过 dubbo rpc 回调注册方来执行任务。 注册方执行完后会回调调度中心更新任务状态。 调研调研时发现，目前开源的比较流行的调度中心主要有两种方案。一种是中心化的，例如xxl-job。另外一种是去中心化的，例如 elastic-job。 由于公司内部zookeeper的使用已经比较成熟了，而且对高可用的要求比较高。于是着重研究了es-job。 es-job实现分片的核心在于利用zk作为协调者来进行分片。 关键流程如下： 1、每一个服务启动时，向zk的worker节点注册，同时参与leader的选举。 2、leader选举成功后，在需要进行分片的任务执行时，获取worker节点的数量，对任务进行切分，分配至worker节点下。 3、worker节点执行任务时从cache读取需要执行的任务进行执行，同时更新任务执行的状态。 关键的流程是比较简单的，主要就是利用了zk。 新架构设计我们公司的调度产品有个特点，底层数据库就是分表的，固定的256张表，那么存在天然的分片优势。 于是，我们在设计时也就比较简单，将256张表分给注册在zk worker节点下的worker就行。 实现1、调度中心起来后，向zk 注册 worker临时节点，同时监听链接状态和节点 node cache 的变化。 2、每个实例都参与 leader 选举。成为leader 的实例 对 worker 根节点添加监听器，监听 worker 变化事件。 3、leader 根据 worker 数量，对 256 张表进行分配，分配结果写入到 node cache。 4、worker 监听到 node cache 发生变化，获取最新数据， 写入本地的缓存。 5、当 worker 与 zk 之间连接发生波动时，清空本地缓存。 6、调度扫描线程根据本地缓存里的表，进行扫表执行。 效果老架构CPU监控 新架构CPU监控 可以看到，一人干活，他人围观的场面一去不复返了。大家一起干活，很和谐。 探索etc","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}],"tags":[]},{"title":"dubbo调用在2.7.5版本以下的性能问题的解决","slug":"dubbo调用在2.7.5版本以下的性能问题的解决","date":"2020-06-11T02:26:30.501Z","updated":"2020-06-24T06:59:04.285Z","comments":true,"path":"2020/06/11/dubbo-call-to-solve-performance-problems-below-version-275.html","link":"","permalink":"http://xingcici.github.io/2020/06/11/dubbo-call-to-solve-performance-problems-below-version-275.html","excerpt":"","text":"背景toc 的监控在日常环境测试时，发现日常环境在大量JOB需要调度的情况下，发现三台机器调度的瓶颈大概为8k每分钟，也就是一台机器每秒处理44个，平均每个JOB耗时22毫秒。这还是在已经分片的情况下。 如果只是 dubbo 调用就要这么久肯定不正常。于是进行排查。 数据理论上最耗时的点应该是在远程调用的过程，也就是网络延迟。在日常环境用arthas抓取方法耗时，发现耗时的过程反而是在 这个方法上。 用 arthas 抓的图如图所示。 可以看到 config.get 这个方法远大于或者接近实际调用过程。理论上对这个点进行优化可以至少提升一倍的性能。 解决过程继续分析 get 方法，其内部耗时主要还是在 checkAndUpdateSubConfigs(); 这个方法。 这个方法主要是获取刷新各种配置。而且由于我们没有用新版的dubbo配置中心，导致每调一次GET会打一次warn日志 于是考虑要不在get 做个缓存来解决。但是在解决之前也查一下有没有其他人碰到这个问题，结果还真有。他们解决的办法是升级dubbo 版本。apache 2.7.5 版本 就解决了这个性能问题，将 checkAndUpdateSubConfigs 丢到 init 方法里，只在 init 的时候调用一次。 不过由于升级 dubbo 核心版本没那么容易，于是暂时就在 get 外层做了个缓存来解决。 下图是修改后的耗时，可以看到耗时基本只剩调用这块。具体的性能提升多少等待后续的测试。","categories":[{"name":"日常问题","slug":"日常问题","permalink":"http://xingcici.github.io/categories/%E6%97%A5%E5%B8%B8%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://xingcici.github.io/tags/dubbo/"}]},{"title":"Sentinel生产环境改造实践","slug":"Sentinel生产环境改造实践","date":"2020-04-08T05:23:50.763Z","updated":"2020-06-24T06:59:04.277Z","comments":true,"path":"2020/04/08/sentinel-production-environment-transformation-practice.html","link":"","permalink":"http://xingcici.github.io/2020/04/08/sentinel-production-environment-transformation-practice.html","excerpt":"","text":"引言随着业务的发展，公司的服务需要对某些接口进行限流或者根据某些属性对请求进行限制。之前的方式是将黑白名单存储在数据库或者配置中心，无法较快和方便得对请求进行限制。公司安排平台组进行相关的调研，调研的结果显示阿里开源的 Sentinel 最符合我们目前的需求。根据文档，目前开源的版本v1.6.3无法在生产环境直接使用。根据公司目前的情况，我们只需要使用网关部分的功能，要进行以下几点改造。1.监控数据的持久化。2.网关流控规则和API组规则接入 Apollo 配置中心。3.登录鉴权体系。本文主要介绍在改造过程中的实践。 实践Sentinel 网关限流架构在实践开始之前，先了解一下 Sentinel 网关限流的架构。如下图所示。 惠借准备将原先的 Zuul 1.x 网关升级成 Spring Cloud Gateway，本文就以Spring Cloud Gateway 为例，以下简称 SCG。SCG中需要加入sentinel适配SCG的jar包，并且在启动时加入一些配置参数。 SCG启动时会向 Dashboard 服务请求API组规则和流控规则，加入到本机的配置中。 在请求经过SCG时会对请求进行统计和输出日志。 Dashboard 更新规则时会更新本地内存中的规则并推送至SCG。 Dashboard 会定时拉取SCG输出的日志的统计结果。 监控数据持久化在上述架构图中，集成 Sentinel SCG jar包后，Sentinel 会在 SCG的过滤器中加入一个 SentinelGatewayFilter。对 SCG 请求的拦截就由这个过滤器来实现。具体监控数据的生成和输出由一系列的 Slot 负责，具体原理这里不展开叙述，因为本文主要描述生产环境使用的改造，有兴趣的同学可以搜一下 Sentinel 时间窗口。原先的监控数据由 Dashboard 收集后存储在 Map 中，并且只存储五分钟内的数据，在 Dashboard 重启后监控数据就消失，这在生产环境中肯定是不能接受的。由于我们公司本身也有ES服务，这些数据又是个时序数据，自然而然就想到可以放入ES中进行持久化。首先新建 ElasticsearchMetricsRepository 来实现 MetricsRepository 接口类。分别实现其中的 save、saveAll、queryByAppAndResourceBetween 和 listResourcesOfApp。在 MetricController、 MetricFetcher 中把自动注入的 MetricsRepository 加上 @Qualifier(value = &quot;elasticsearchMetricsRepository&quot;)。还有另外一个点在于 Dashboard 查询监控数据进行展示时，开源的版本并不会将调用量为零的那段数据补齐，导致监控数据展示得非常奇怪。所以我们需要在查询监控数据接口对ES返回的数据进行补齐，让监控数据是连续的。下图是改造完的监控。 网关流控规则和 API 规则接入Apollo配置中心开源版本的 Sentinel 流控规则和API规则也都是内存态的，在 SCG 和 Dashboard 重启后也会消失。我们原先的配置中心采用的就是 Apollo，Sentinel 刚好可以跟 Apollo 进行集成。实现 Dashboard 推送至 Apollo，Apollo 再推送至 SCG，SCG启动时也从 Apollo 进行加载。 SCG方面跟普通应用接入 Apollo 一样，在 Apollo 建立应用。同时需要新增两个特殊的 namespace，分别为 flow_rule 、 api_group。在SCG中加入 Sentinel的 Apollo 依赖，同时配置一般的 Apollo 参数。然后需要对 Sentinel 的流控规则和API组规则加载方式进行修改。在 SentinelGatewayConfiguration 配置中加入如下两段代码 后续加载规则分别从某 namespace 下的某 key 加载。 Dashboard方面Dashboard 这边本身 Sentinel 已经把普通应用的流控规则从 Apollo 加载的代码已经写好放在 test 目录下，但是没有实现网关的代码。其实也很简单，Sentinel 的数据加载和推送的方式本身就提供了较好的扩展性。我们只需要分别实现 DynamicGatewayRuleProvider 和 DynamicGatewayRulePublisher。这里需要注意的一个点是 SCG 向 Dashboard 注册的 appName 最好和它本身在 Apollo 的 appId 保持一致。这样 DynamicGatewayRuleProvider在加载数据的时候直接用 appName 作为 appId 向 Apollo 请求配置。加载流控规则代码如下图所示。 至于 DynamicGatewayRulePublisher 其实有两个过程，第一步是向Apollo更新数据，第二步是要求 Apollo 对该 namespace 进行发布。代码如下图所示。 这里面涉及到的 Apollo 开放平台的授权之类的过程（这次也发现这是Apollo的一个非常强大的功能，实际上可以有很多的应用，特别是在自动化方面）请参考 Apollo 的文档。在这样的一个过程后很多同学应该能很自然而然地想到是不是可以在 Sentinel 或者其他监控工具监控到某些条件后自动地对 Apollo 的中的限流配置进行修改。是的，这也应该是我们以后的目标，做到限流的自动化和智能化。随后还需要新增 GatewayApiControllerV2，GatewayFlowRuleControllerV2，基本上就是把原有的GatewayApiController、GatewayFlowRuleController复制过来， 然后分别注入 ApolloProvider 和 ApolloPublisher。Controller 要改动的地方主要有两个，第一个将获取规则的方式从向SCG请求改为向 Apollo 请求，第二个是新增和修改规则先更新到 Apollo 再让Apollo 推送到 SCG。再将前端页面请求的路径指向新的 Controller。下图是获取API规则的代码改造前和改造后区别。ApiDefinitionProvider 就是 DynamicGatewayApiDefinitionProvider 的从 Apollo 获取 API 规则的实现。 此处还有个暗坑，一开始没注意到。本身Dashboard在新增规则时会生成一个ID，这个ID是个 AtomicLong, 随着Dashboard的重启自动就清空了，导致在重启后新增或修改规则会将ID=1的规则顶替。目前解决的办法是在新增和修改规则时加载目前的全部规则，然后在获取ID时获取当前全部规则的最大ID，从最大值开始递增。 登录和鉴权原先的登录和鉴权只提供了基础的登录功能且安全性很低。我们将其加入了公司的统一登录中心，实现了帐号密码及企业微信扫码登录和访问权限的控制。 尾声Sentinel 的改造陆陆续续进行了半个多月，其中也遇到了不少了问题，基本上都得到了较好的解决。目前仅仅是生产环境能用的状态，还有很多功能尚未完善。在改造的过程中也对Sentinel的基本原理和架构有了较清晰的认识，为后续使用中解决问题提供了基础。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[]},{"title":"针对流量的AB test 的一点思考","slug":"针对流量的AB test 的一点思考","date":"2020-04-08T05:19:40.257Z","updated":"2020-06-24T06:59:04.263Z","comments":true,"path":"2020/04/08/some-thoughts-on-ab-test-for-traffic.html","link":"","permalink":"http://xingcici.github.io/2020/04/08/some-thoughts-on-ab-test-for-traffic.html","excerpt":"","text":"之前公司提了个需求，不同比例的用户导向不同的落地页，统计转化效率。我调研了一些业界的方案后有了一些自己的思考。 在阅读下述方案之前 我们需要明确几个定义 AB TEST 方案分版本，不同的 AB TEST 方案拥有不同的标识，例如 ABTEST_2020_01_01。 某个确定的 AB TEST 版本下，会有不同的分组，每个分组也有确定的标识，例如 GROUP_A。下文中的 AB TEST 桶 ，指的就是不同的分组。 流量进入后会被打上一个或者多个 AB TEST 版本标识及 AB TEST 桶标识。 单纯针对该需求的方案目标: 相同用户返回相同落地页 不同用户根据比例返回不同落地页 跟踪不同落地页转化效率 方案: 根据ABTEST版本 建立唯一标识, 请求跳转链接时根据用的IP或者CookieId进行HASH，根据HASH命中区间( AB TEST 桶)返回不同的落地页，返回结果时带上TEST版本标识，转化效率的跟踪在H5进行。同时留有后门方便指定用户分入不同的区间中。 更通用的方案(也只是简略描述 具体还要根据详细的需求来细化)目标: 根据不同用户特征返回不同结果，跟踪不同返回结果的转化效率。 思路A 流量完全分层 所有流量可以循环利用 一个请求可以用于多个 AB TEST 方案: 设置要针对的特征，圈定范围用户。比如 用户的IP 用户的 OPENID 设置 AB TEST 方案，每个AB TEST方案都有唯一标识。再在 AB TEST 方案中，根据不同特征或者比例设置不同的 AB TEST 桶，每个 AB TEST 桶都有一个唯一标识用于跟踪，每个 AB TEST 桶可以设置一定比例的流量。 在网关设置过滤器，对进入的符合要求的流量特征进行HASH(同时也可以针对特定的用户标识强行进行分组,方便测试)，打上进行中的 AB TEST 方案的标识， 再根据该 AB TEST 方案打上 AB TEST 桶的标识。再对该流量进行下一次循环，查询是否还有 AB TEST 方案可以利用它，有的话则继续进行标记。符合最后的标记可能是类似于TESTA_GROUP1|TESTB_GROUP2。 流量到服务的时候，根据标识返回不同的结果，在返回的同时带上 AB TEST 方案的标识。 存在的问题：当不同的 AB TEST 方案对相同页面做实验时就会出现问题。 思路B 流量预先分桶 一个请求只能用于一个 AB TEST 方案: 设置要针对的特征，圈定范围用户。比如 用户的IP 用户的 OPENID。 设置实验方案，每个 AB TEST 方案都有每一标识。再在 AB TEST 方案中，根据不同特征或者比例设置不同的 AB TEST 桶，每个 AB TEST 桶都有一个唯一标识用于跟踪。 在网关设置过滤器，对圈定范围流量先随机进行HASH，按实验需要的流量大小随机分到不同的流量桶中，再在流量桶中实施具体的 AB TEST 方案， 再根据该 AB TEST 方案打上 AB TEST 桶的标识 例如 TESTA_GROUP1，随后直接进入服务端。 流量到服务的时候，根据标识返回不同的结果，在返回的同时带上 AB TEST 方案的标识。 存在的问题：当 AB TEST 方案逐渐增加时，后来增加的 AB TEST 方案可能无法获取到足够的流量 附图","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}],"tags":[]},{"title":"中小型公司数仓摸索和应用","slug":"中小型公司数仓摸索和应用","date":"2020-04-08T05:12:24.879Z","updated":"2020-06-24T06:59:04.261Z","comments":true,"path":"2020/04/08/exploration-and-application-of-data-warehouse-for-small-mediumsized-companies.html","link":"","permalink":"http://xingcici.github.io/2020/04/08/exploration-and-application-of-data-warehouse-for-small-mediumsized-companies.html","excerpt":"","text":"前言熬过创业初期的互联网公司一般发展迅速。一方面是业务的迅速发展，另一方面是数据的迅速累积。在获客成本越来越高的当下，谁利用好手中积累的数据来更好的服务于业务的发展，做到精细化运营，谁就能在竞争中获得领先优势。 对分析数据而言，数仓是必不可少的基础。在惠借科技之前的两年里，数据分析方面并没有一个清晰的数仓这样的概念，导致数据分析过程中一系列例如数据来源不一致，表过多和界限混乱等问题。最终导致的结果就是对业务响应慢，数据可能不准确。所有重新分析和搭建适合惠借的数仓是迫在眉睫的事情。惠借技术团队和BI团队经过两三个月，也算是初步完成了数仓的选型和分层。在此简单介绍下整个过程。 现状和需求分析目前公司内部BI团队是应用阿里云的maxcompute作为数据分析和平台。应用场景大部分是对用户部分行为进行分析和对订单进行分析，集群配置为10CU，数据量为几百GB，每天新增数据量最多几十GB。新的技术和平台肯定要能支撑起目前的业务需求同时能部分满足未来的扩展需求。 技术和平台选型正如前言所言，我们在选型时也面临着自建和使用第三方平台的问题。我们在一开始进行技术选型时也优先考虑自建Hadoop集群，并用Cloudera Manager在测试环境搭建了一套Hadoop环境。不得不说现在搭建一套Hadoop环境还是很方便的。Cloudera Manager集成了一系列的部署、配置和监控等功能。 然而并不是搭建一个Hadoop环境就万事大吉了。在Hadoop的调研和测试使用过程中，我们发现Hadoop对集群机器的配置要求很高，而且还有各种复杂的配置。其中配置又是特别重要的部分，需要针对不同的机器配置和实际情况进行配置才能达到最优的效果。现实情况是，我们目前并没有专业的偏向于这方面维护和调优的工程师，这就意味着自建的话在以后的使用过程中遇到各种问题的话，会有较高的维护和解决问题的成本。 我们转而将视线放在目前我们正在使用的 Maxcompute。看了Maxcompute文档后发现是阿里云基于Hadoop开发的云上大数据平台。我们目前服务器、线上数据库及日志都是采用阿里云提供的服务，而Maxcompute提供了一系列的与阿里云目前的产品配合的功能。例如数据直接从RDS导入，日志文件由LogHub投递到Maxcompute等。很大程度上方便我们的使用。同时减少了BI同学的学习成本。而后在经济成本方面进行了简单的计算发现，在我们目前的需要的配置下，直接使用Maxcompute的成本要低于自建Hadoop成本。 最后我们决定基于阿里云Maxcompte来搭建我们新的数仓体系。 数仓设计 上图就是我们目前的第一版数仓的设计。因为采用了Maxcompute，所以我们目前并不需要特别关注存储层的设计，而是专注于数仓层的设计。 数仓最底层是 近源数据层，该层的数据直接从各种数据源进入，仅仅进行一些简单的ETL过程。一般来说会直接和外部数据源的结构进行映射。 在近源数据层上方是 基础数据层，基础数据层的数据从近源数据抽取，经过清洗和转化后形成我们设计好的基础表等。 基础数据上方为宽表和主题表，该层的数据由基础数据层转化和组合而成。表的主题由BI同学对业务进行分析后得到。 最后数据形成 数据集市，这是能够直接满足业务需求的数据。这里的数据可以导出到RDS中提供给业务方使用。也可以提供给BI分析工具使用，例如我们计划使用的 SuperSet BI分析工具等。 规范其实在之前我们也是用 Maxcompute，那为什么还是需要重新构建呢。因为我们之前在使用的过程中，并没有参考数仓的一般规范及阿里云的Maxcompute最佳实践，导致使用过程中产生很多问题。最简单的例子就是表的命名没有遵循分层的命名规范，命名很混乱。所以定义 一系列的规范来约束开发过程是十分必要的,我们公司内部的规范仍在不断地完善中。 后话至此初步的数仓搭建已经结束，这是我们数仓的1.0版本，后续随着业务的变化和发展，数仓也会进行相应的调整来满足需求。","categories":[{"name":"数据智能","slug":"数据智能","permalink":"http://xingcici.github.io/categories/%E6%95%B0%E6%8D%AE%E6%99%BA%E8%83%BD/"}],"tags":[]}],"categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"加密","slug":"加密","permalink":"http://xingcici.github.io/categories/%E5%8A%A0%E5%AF%86/"},{"name":"源码","slug":"源码","permalink":"http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"},{"name":"数据智能","slug":"数据智能","permalink":"http://xingcici.github.io/categories/%E6%95%B0%E6%8D%AE%E6%99%BA%E8%83%BD/"},{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"},{"name":"编程","slug":"编程","permalink":"http://xingcici.github.io/categories/%E7%BC%96%E7%A8%8B/"},{"name":"架构设计","slug":"架构设计","permalink":"http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"},{"name":"分布式","slug":"分布式","permalink":"http://xingcici.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"日常问题","slug":"日常问题","permalink":"http://xingcici.github.io/categories/%E6%97%A5%E5%B8%B8%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://xingcici.github.io/tags/RocketMQ/"},{"name":"https","slug":"https","permalink":"http://xingcici.github.io/tags/https/"},{"name":"jdk","slug":"jdk","permalink":"http://xingcici.github.io/tags/jdk/"},{"name":"MQ","slug":"MQ","permalink":"http://xingcici.github.io/tags/MQ/"},{"name":"hadoop","slug":"hadoop","permalink":"http://xingcici.github.io/tags/hadoop/"},{"name":"链路追踪","slug":"链路追踪","permalink":"http://xingcici.github.io/tags/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/"},{"name":"密码学","slug":"密码学","permalink":"http://xingcici.github.io/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"JDK","slug":"JDK","permalink":"http://xingcici.github.io/tags/JDK/"},{"name":"反射","slug":"反射","permalink":"http://xingcici.github.io/tags/%E5%8F%8D%E5%B0%84/"},{"name":"linux","slug":"linux","permalink":"http://xingcici.github.io/tags/linux/"},{"name":"网络","slug":"网络","permalink":"http://xingcici.github.io/tags/%E7%BD%91%E7%BB%9C/"},{"name":"Netty","slug":"Netty","permalink":"http://xingcici.github.io/tags/Netty/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://xingcici.github.io/tags/Zookeeper/"},{"name":"算法","slug":"算法","permalink":"http://xingcici.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"dubbo","slug":"dubbo","permalink":"http://xingcici.github.io/tags/dubbo/"}]}