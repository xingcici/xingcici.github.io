{"meta":{"title":"一方天地","subtitle":"","description":"记录生活学习","author":"行词","url":"http://xingcici.github.io","root":"/"},"pages":[{"title":"标签","date":"2020-06-24T07:37:13.614Z","updated":"2020-06-24T07:37:13.614Z","comments":false,"path":"tags/index.html","permalink":"http://xingcici.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-06-24T07:43:54.601Z","updated":"2020-06-24T07:43:54.601Z","comments":false,"path":"categories/index.html","permalink":"http://xingcici.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"零知识证明","slug":"零知识证明","date":"2020-07-13T10:58:34.892Z","updated":"2020-07-13T11:00:05.844Z","comments":true,"path":"2020/07/13/zero-knowledge-proof.html","link":"","permalink":"http://xingcici.github.io/2020/07/13/zero-knowledge-proof.html","excerpt":"","text":"零知识证明 （被称为 “zk-SNARK”）是实现 Zcash 的匿名特性的核心技术。“零知识证明” 的定义是：证明者能够在不向验证者提供任何有用的信息的情况下，使验证者相信某个论断是正确的。举个简单的例子：A 要向 B 证明自己拥有某个房间的钥匙，假设该房间只能用钥匙打开锁，而其他任何方法都打不开。这时有 2 个方法： （一）A 把钥匙出示给 B，B 用这把钥匙打开该房间的锁，从而证明 A 拥有该房间的正确的钥匙。 （二）B 确定该房间内有某一物体，A 用自己拥有的钥匙打开该房间的门，然后把物体拿出来出示给 B，从而证明自己确实拥有该房间的钥匙 后面这个方法属于零知识证明。好处在于在整个证明的过程中，B 始终不能看到钥匙的样子，从而避免了钥匙的泄露。","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://xingcici.github.io/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"}]},{"title":"FutureTask","slug":"FutureTask","date":"2020-07-13T09:44:52.627Z","updated":"2020-07-13T09:47:11.607Z","comments":true,"path":"2020/07/13/futuretask.html","link":"","permalink":"http://xingcici.github.io/2020/07/13/futuretask.html","excerpt":"","text":"我们实现 Callable 接口，在覆写的 call 方法中定义需要执行的业务逻辑； 然后把我们实现的 Callable 接口实现对象传给 FutureTask，然后 FutureTask 作为异步任务提交给线程执行； 最重要的是 FutureTask 内部维护了一个状态 state，任何操作（异步任务正常结束与否还是被取消）都是围绕着这个状态进行，并随时更新 state 任务的状态； 只能有一个线程执行异步任务，当异步任务执行结束后，此时可能正常结束，异常结束或被取消。 可以多个线程并发获取异步任务执行结果，当异步任务还未执行完，此时获取异步任务的线程将加入线程等待列表进行等待； 当异步任务线程执行结束后，此时会唤醒获取异步任务执行结果的线程，注意唤醒顺序是 “后进先出” 即后面加入的阻塞线程先被唤醒。 当我们调用 FutureTask.cancel 方法时并不能真正停止执行异步任务的线程，只是发出中断线程的信号。但是只要 cancel 方法返回 true，此时即使异步任务能正常执行完，此时我们调用 get 方法获取结果时依然会抛出 CancellationException 异常。 利用 LockSupport 来实现线程的阻塞 \\ 唤醒机制； 利用 volatile 和 UNSAFE 的 CAS 方法来实现线程共享变量的无锁化操作； 若要编写超时异常的逻辑可以参考 FutureTask 的 get(long timeout, TimeUnit unit) 的实现逻辑； 多线程获取某一成员变量结果时若需要等待时的线程等待链表的逻辑实现； 某一异步任务在某一时刻只能由单一线程执行的逻辑实现； FutureTask 中的任务状态 state 的变化处理的逻辑实现。","categories":[{"name":"源码","slug":"源码","permalink":"http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"http://xingcici.github.io/tags/JDK/"}]},{"title":"RockeMQ 单机版在 centos 上的部署","slug":"RockeMQ 单机版在 centos 上的部署","date":"2020-06-30T10:14:15.737Z","updated":"2020-07-01T05:34:11.679Z","comments":true,"path":"2020/06/30/rockemq-standalone-version-deployment-on-centos.html","link":"","permalink":"http://xingcici.github.io/2020/06/30/rockemq-standalone-version-deployment-on-centos.html","excerpt":"","text":"最近需要需要再研究下 RocketMQ 的文件系统的具体实现，于是重新在自己的服务器上安装了一遍，记录下过程。 到镜像站下载安装包 wget http://mirror.bit.edu.cn/apache/rocketmq/4.7.1/rocketmq-all-4.7.1-bin-release.zip 并解压缩 修改 broker.conf 文件（conf/broker.conf）添加 naemServer 地址的属性以及自动创建 Topic 的属性。 namesrvAddr = 127.0.0.1:9876 autoCreateTopicEnable = true 修改启动参数（由于 rocketMQ 对内存的消耗比较大，所以测试时修改为本机合适的大小）。主要修改 bin 目录下的 runserver.sh 和 runbroker.sh 下的 JAVA_OPT 属性。 nohup sh bin/mqnamesrv &amp; nohup sh bin/mqbroker -c /root/download/rocketmq/conf/broker.conf &amp; sh bin/mqshutdown namesrv sh bin/mqshutdown broker 注意开放端口 有几个坑要注意 开放端口 我在部署的时候碰到 store 目录下没法自动建文件夹的问题。 broker.conf 增加配置 namesrvAddr = 127.0.0.1:9876brokerIP1=47.101.33.17autoCreateTopicEnable = true","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://xingcici.github.io/tags/RocketMQ/"}]},{"title":"反射获取一个类的私有方法","slug":"反射获取一个类的私有方法","date":"2020-06-24T09:29:25.561Z","updated":"2020-06-30T03:33:21.715Z","comments":true,"path":"2020/06/24/reflection-to-get-a-class-private-method.html","link":"","permalink":"http://xingcici.github.io/2020/06/24/reflection-to-get-a-class-private-method.html","excerpt":"","text":"比较简单 Github 123456789101112131415161718192021222324public class AccessPrivateMember &#123; public static void main(String[] args) &#123; try &#123; Class c = Class.forName(\"com.example.reflect.HelloService\"); //能获取所有有访问权限的方法，包括父类中继承的 Method publicMethod = c.getMethod(\"publicHello\", String.class); Method saySomething = c.getMethod(\"publicHello\", String.class); //获取所有方法 本方法中 Method thisClassMethod = c.getDeclaredMethod(\"privateHello\", String.class); //设置权限 thisClassMethod.setAccessible(true); //不能直接转成类来执行 classloader 不同 publicMethod.invoke(c.newInstance(), \"publicMethod\"); saySomething.invoke(c.newInstance(), \"saySomething\"); thisClassMethod.invoke(c.newInstance(), \"thisClassMethod\"); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"编程","slug":"编程","permalink":"http://xingcici.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"反射","slug":"反射","permalink":"http://xingcici.github.io/tags/%E5%8F%8D%E5%B0%84/"}]},{"title":"RocketMQ 消息存储的设计与实现","slug":"RocketMQ 消息存储的设计与实现","date":"2020-06-24T09:01:25.741Z","updated":"2020-07-01T07:10:45.920Z","comments":true,"path":"2020/06/24/design-and-implementation-of-rocketmq-message-storage.html","link":"","permalink":"http://xingcici.github.io/2020/06/24/design-and-implementation-of-rocketmq-message-storage.html","excerpt":"","text":"作为一款高性能的消息中间件，RocketMQ 基于互联网的生产要求对多 Topic 场景做了诸多针对性优化。根据中间件团队提供的压测报告，在 Producer 和 Consumer 共存的情况下，相比于 Kafka，RocketMQ 的性能指标（TPS 和 RT）随着 Topic 数量的上升表现稳定。本文从消息存储的角度谈谈 RocketMQ 高性能的原因，重点包括四个方面：消息文件存储的结构设计、消息从 Broker 接收到持久化磁盘的流程、刷盘策略和内存映射优化机制。 消息文件存储结构设计消息持久化刷盘策略内存映射","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://xingcici.github.io/tags/RocketMQ/"}]},{"title":"深入理解 JAVA 反序列化漏洞","slug":"深入理解 JAVA 反序列化漏洞","date":"2020-06-24T01:47:01.053Z","updated":"2020-06-24T06:59:04.258Z","comments":true,"path":"2020/06/24/indepth-understanding-of-java-deserialization-vulnerability.html","link":"","permalink":"http://xingcici.github.io/2020/06/24/indepth-understanding-of-java-deserialization-vulnerability.html","excerpt":"","text":"惊闻 dubbo 爆出严重的反序列化漏洞 CVE-2020-1948：Apache Dubbo 远程代码执行漏洞通告 加上之前的 fastjson 不断的反序列化漏洞，于是了解了下这方面的知识。详见深入理解 JAVA 反序列化漏洞","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"tags":[]},{"title":"Linux命令学习","slug":"Linux命令学习","date":"2020-06-23T06:16:36.652Z","updated":"2020-06-24T06:59:04.203Z","comments":true,"path":"2020/06/23/linux-command-learning.html","link":"","permalink":"http://xingcici.github.io/2020/06/23/linux-command-learning.html","excerpt":"","text":"Linux命令大全","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://xingcici.github.io/tags/linux/"}]},{"title":"编程中的一些感悟","slug":"编程中的一些感悟","date":"2020-06-18T12:20:02.180Z","updated":"2020-06-24T06:59:04.236Z","comments":true,"path":"2020/06/18/some-sentiments-in-programming.html","link":"","permalink":"http://xingcici.github.io/2020/06/18/some-sentiments-in-programming.html","excerpt":"","text":"加强对象意识之前虽然有意识在强化，但是有时候还是写出简单的命令式的代码。对同一个事物的一系列操作，基本都能以该事物发起操作的形式来设计，包装一下。 lifecycle最近开始真的写中间件才意识到，对对象的生命周期的掌握非常重要，下面的这种形式就很方便 1234567891011public class Server implements LifeCycle &#123; @Override public void startup() &#123; &#125; @Override public void shutdown() &#123; &#125;&#125;","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}],"tags":[]},{"title":"Sofa-bolt的简单使用","slug":"Sofa-bolt的简单使用","date":"2020-06-18T03:48:20.319Z","updated":"2020-06-24T06:59:04.218Z","comments":true,"path":"2020/06/18/simple-use-of-sofabolt.html","link":"","permalink":"http://xingcici.github.io/2020/06/18/simple-use-of-sofabolt.html","excerpt":"","text":"代码已经放在 github","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://xingcici.github.io/tags/%E7%BD%91%E7%BB%9C/"},{"name":"Netty","slug":"Netty","permalink":"http://xingcici.github.io/tags/Netty/"}]},{"title":"由No route info for this topic引发的关于RocketMQ的问题","slug":"由No route info for this topic引发的关于RocketMQ的问题","date":"2020-06-17T01:21:44.351Z","updated":"2020-06-24T06:59:04.329Z","comments":true,"path":"2020/06/17/questions-about-rocketmq-caused-by-no-route-info-for-this-topic.html","link":"","permalink":"http://xingcici.github.io/2020/06/17/questions-about-rocketmq-caused-by-no-route-info-for-this-topic.html","excerpt":"","text":"背景昨天一位业务的同学在我审批MQ通过后，在业务代码里加了个 producer，结果启动项目时集成的调度中心的二方包里的 producer 在通过 MQ 注册 JOB 时报 No route info for this topic。 思路这个异常也算比较常见了，一般是没有创建 topic，或者连接错了 namesrv 导致。但是查看了他的代码后发现配置没有问题。然后他反馈他 DEBUG 出来 producer 里的 namesrv 是正确的，然后他自己新加的。这就很奇怪了，于是我让他提交了代码然后申请了代码权限看下原因。 解决这里先说一句，我对 BUG 这种东西，一直都相信一句话，99%你碰到觉得无比高深的 BUG，基本都是非常简单的点弄错了导致的。这次果然还是如此。 一开始我都没去看配置的问题，因为从业务方之前截图给我的配置里是没问题的。我们对配置的引用有两种方式，一种是 @key@，另外一种是 ${key}。截图给我的时候用的是 @key@，我想当然觉得这应该也是能正常拿到 value 的。 于是开始DEBUG，发现他配置的 @key@ 居然没引用到 value，直接把 @key @作为 namesrv 传进去了,如下图所示。 但是思考一下，又有问题了。按正常的思维，namesrv配置难道不是应该跟着 producer 吗。难道 rocketMQ 在同一个 JVM 中只允许连一个 namesrv?如果是这样，那是配置的覆盖还是单例来实现呢? 于是继续看源码。 异常是在这行抛出 抛出的原因是下面这行判断不通过。 1topicPublishInfo !&#x3D; null &amp;&amp; topicPublishInfo.ok() 继续追踪下去，可以看到是这个 Map 中 topic 对应的 topic 路由信息不存在。 那么这个Map又是什么时候写入信息的呢。其实是在下图这个地方。 上面那个写入路由信息的方法又是在这个地方调用的。注意，这个方法是 MQClientInstance 里的，不知道大家看到 Instance 这个类名结尾有没有啥感觉。我是看到这个基本就往单例或者跟某个 key 绑定这方面去想。那么继续看 MQClientInstance 是怎么维护的。 MQClientInstance 存储和生成的地方如下面两张图所示。 可以看到是在这个地方做了个绑定，那么继续往下看。生成了 MQClientInstance 并以下图里的代码的结果为 key 放入 那真相就出现了，同一个 JVM 中如果配置的时候不配 unitName，那么不管配多少个 producer 都只会有一个 MQClientInstance,相当于 config 也就生效一份 😁。基本就看哪个 producer先初始化了。后面来的只能拿之前的那个 MQClientInstance 来获取路由信息。 再回到之前最开始的问题，业务方后面配置的 producer 是用 xml 配置的 bean，而调度中心的 producer 他们使用时在重写了 afterPropertiesSet 方法里初始化。理论上肯定是 spring 自身维护的 bean 初始化在 afterPropertiesSet 之前。然而xml里配置的value恰好又是错误的，就导致了 No route info for this topic 出现了。将 namesrv 的引用改为正确的，问题解决。 后续在 vdianmq 里可以以 warn log 的形式提醒下业务方这方面使用时需要注意一下。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://xingcici.github.io/tags/RocketMQ/"}]},{"title":"Zookeepr在分布式协调中的使用","slug":"Zookeepr在分布式协调中的使用","date":"2020-06-16T03:00:46.686Z","updated":"2020-06-24T07:11:58.541Z","comments":true,"path":"2020/06/16/use-of-zookeepr-in-distributed-coordination.html","link":"","permalink":"http://xingcici.github.io/2020/06/16/use-of-zookeepr-in-distributed-coordination.html","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://xingcici.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://xingcici.github.io/tags/Zookeeper/"}]},{"title":"Sofa-bolt源码阅读","slug":"Sofa-bolt源码阅读","date":"2020-06-16T02:57:12.287Z","updated":"2020-06-24T06:59:04.233Z","comments":true,"path":"2020/06/16/sofabolt-source-reading.html","link":"","permalink":"http://xingcici.github.io/2020/06/16/sofabolt-source-reading.html","excerpt":"","text":"​ SOFABolt 是蚂蚁金融服务集团开发的一套基于 Netty 实现的网络通信框架。 为了让 Java 程序员能将更多的精力放在基于网络通信的业务逻辑实现上，而不是过多的纠结于网络底层 NIO 的实现以及处理难以调试的网络问题，Netty 应运而生。 为了让中间件开发者能将更多的精力放在产品功能特性实现上，而不是重复地一遍遍制造通信框架的轮子，SOFABolt 应运而生。 Bolt 名字取自迪士尼动画 - 闪电狗，是一个基于 Netty 最佳实践的轻量、易用、高性能、易扩展的通信框架。 这些年我们在微服务与消息中间件在网络通信上解决过很多问题，积累了很多经验，并持续的进行着优化和完善，我们希望能把总结出的解决方案沉淀到 SOFABolt 这个基础组件里，让更多的使用网络通信的场景能够统一受益。 目前该产品已经运用在了蚂蚁中间件的微服务 (SOFARPC)、消息中心、分布式事务、分布式开关、以及配置中心等众多产品上。","categories":[{"name":"源码","slug":"源码","permalink":"http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://xingcici.github.io/tags/%E7%BD%91%E7%BB%9C/"},{"name":"Netty","slug":"Netty","permalink":"http://xingcici.github.io/tags/Netty/"}]},{"title":"Raft算法的简单实现","slug":"Raft算法的简单实现","date":"2020-06-15T12:02:55.454Z","updated":"2020-06-24T06:59:04.235Z","comments":true,"path":"2020/06/15/simple-implementation-of-raft-algorithm.html","link":"","permalink":"http://xingcici.github.io/2020/06/15/simple-implementation-of-raft-algorithm.html","excerpt":"","text":"敬请期待","categories":[{"name":"分布式","slug":"分布式","permalink":"http://xingcici.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://xingcici.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"调度中心任务分片方案设计和实现","slug":"调度中心任务分片方案设计和实现","date":"2020-06-12T06:26:50.069Z","updated":"2020-06-24T06:59:04.262Z","comments":true,"path":"2020/06/12/design-and-implementation-of-task-sharding-scheme-in-dispatch-center.html","link":"","permalink":"http://xingcici.github.io/2020/06/12/design-and-implementation-of-task-sharding-scheme-in-dispatch-center.html","excerpt":"","text":"背景微店任务调度中心（TOC）每天承担千万甚至亿量级的任务调度，不单单是循环的传统任务调度，还承担了类似于延时消息的一次性调度。但是目前设计上仍然比较原始，是TOC抢分布式锁后通过线程不断扫表读取需要调度的job，再通过 dubbo 调度job，形成了一人工作，他人围观的场景。虽然能满足目前的需求，但是已经越来越力不从心，在一些极端的情况下，比如某个应用注册任务激增的情况下，就会出现大量的调度延迟。于是以能无限水平扩容为目标的调度方案的改造势在必行。 老架构设计服务通过抢占zk分布式锁来获得执行权限，从数据库中扫表获得需要执行的任务。因为我们有256张分表，我们也限制成从每张分表里读取100条记录，每次最大总数也就是25600。 执行时通过 dubbo rpc 回调注册方来执行任务。 注册方执行完后会回调调度中心更新任务状态。 调研调研时发现，目前开源的比较流行的调度中心主要有两种方案。一种是中心化的，例如xxl-job。另外一种是去中心化的，例如 elastic-job。 由于公司内部zookeeper的使用已经比较成熟了，而且对高可用的要求比较高。于是着重研究了es-job。 es-job实现分片的核心在于利用zk作为协调者来进行分片。 关键流程如下： 1、每一个服务启动时，向zk的worker节点注册，同时参与leader的选举。 2、leader选举成功后，在需要进行分片的任务执行时，获取worker节点的数量，对任务进行切分，分配至worker节点下。 3、worker节点执行任务时从cache读取需要执行的任务进行执行，同时更新任务执行的状态。 关键的流程是比较简单的，主要就是利用了zk。 新架构设计我们公司的调度产品有个特点，底层数据库就是分表的，固定的256张表，那么存在天然的分片优势。 于是，我们在设计时也就比较简单，将256张表分给注册在zk worker节点下的worker就行。 实现1、调度中心起来后，向zk 注册 worker临时节点，同时监听链接状态和节点 node cache 的变化。 2、每个实例都参与 leader 选举。成为leader 的实例 对 worker 根节点添加监听器，监听 worker 变化事件。 3、leader 根据 worker 数量，对 256 张表进行分配，分配结果写入到 node cache。 4、worker 监听到 node cache 发生变化，获取最新数据， 写入本地的缓存。 5、当 worker 与 zk 之间连接发生波动时，清空本地缓存。 6、调度扫描线程根据本地缓存里的表，进行扫表执行。 效果老架构CPU监控 新架构CPU监控 可以看到，一人干活，他人围观的场面一去不复返了。大家一起干活，很和谐。 探索etc","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}],"tags":[]},{"title":"dubbo调用在2.7.5版本以下的性能问题的解决","slug":"dubbo调用在2.7.5版本以下的性能问题的解决","date":"2020-06-11T02:26:30.501Z","updated":"2020-06-24T06:59:04.285Z","comments":true,"path":"2020/06/11/dubbo-call-to-solve-performance-problems-below-version-275.html","link":"","permalink":"http://xingcici.github.io/2020/06/11/dubbo-call-to-solve-performance-problems-below-version-275.html","excerpt":"","text":"背景toc 的监控在日常环境测试时，发现日常环境在大量JOB需要调度的情况下，发现三台机器调度的瓶颈大概为8k每分钟，也就是一台机器每秒处理44个，平均每个JOB耗时22毫秒。这还是在已经分片的情况下。 如果只是 dubbo 调用就要这么久肯定不正常。于是进行排查。 数据理论上最耗时的点应该是在远程调用的过程，也就是网络延迟。在日常环境用arthas抓取方法耗时，发现耗时的过程反而是在 这个方法上。 用 arthas 抓的图如图所示。 可以看到 config.get 这个方法远大于或者接近实际调用过程。理论上对这个点进行优化可以至少提升一倍的性能。 解决过程继续分析 get 方法，其内部耗时主要还是在 checkAndUpdateSubConfigs(); 这个方法。 这个方法主要是获取刷新各种配置。而且由于我们没有用新版的dubbo配置中心，导致每调一次GET会打一次warn日志 于是考虑要不在get 做个缓存来解决。但是在解决之前也查一下有没有其他人碰到这个问题，结果还真有。他们解决的办法是升级dubbo 版本。apache 2.7.5 版本 就解决了这个性能问题，将 checkAndUpdateSubConfigs 丢到 init 方法里，只在 init 的时候调用一次。 不过由于升级 dubbo 核心版本没那么容易，于是暂时就在 get 外层做了个缓存来解决。 下图是修改后的耗时，可以看到耗时基本只剩调用这块。具体的性能提升多少等待后续的测试。","categories":[{"name":"日常问题","slug":"日常问题","permalink":"http://xingcici.github.io/categories/%E6%97%A5%E5%B8%B8%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://xingcici.github.io/tags/dubbo/"}]},{"title":"Sentinel生产环境改造实践","slug":"Sentinel生产环境改造实践","date":"2020-04-08T05:23:50.763Z","updated":"2020-06-24T06:59:04.277Z","comments":true,"path":"2020/04/08/sentinel-production-environment-transformation-practice.html","link":"","permalink":"http://xingcici.github.io/2020/04/08/sentinel-production-environment-transformation-practice.html","excerpt":"","text":"引言随着业务的发展，公司的服务需要对某些接口进行限流或者根据某些属性对请求进行限制。之前的方式是将黑白名单存储在数据库或者配置中心，无法较快和方便得对请求进行限制。公司安排平台组进行相关的调研，调研的结果显示阿里开源的 Sentinel 最符合我们目前的需求。根据文档，目前开源的版本v1.6.3无法在生产环境直接使用。根据公司目前的情况，我们只需要使用网关部分的功能，要进行以下几点改造。1.监控数据的持久化。2.网关流控规则和API组规则接入 Apollo 配置中心。3.登录鉴权体系。本文主要介绍在改造过程中的实践。 实践Sentinel 网关限流架构在实践开始之前，先了解一下 Sentinel 网关限流的架构。如下图所示。 惠借准备将原先的 Zuul 1.x 网关升级成 Spring Cloud Gateway，本文就以Spring Cloud Gateway 为例，以下简称 SCG。SCG中需要加入sentinel适配SCG的jar包，并且在启动时加入一些配置参数。 SCG启动时会向 Dashboard 服务请求API组规则和流控规则，加入到本机的配置中。 在请求经过SCG时会对请求进行统计和输出日志。 Dashboard 更新规则时会更新本地内存中的规则并推送至SCG。 Dashboard 会定时拉取SCG输出的日志的统计结果。 监控数据持久化在上述架构图中，集成 Sentinel SCG jar包后，Sentinel 会在 SCG的过滤器中加入一个 SentinelGatewayFilter。对 SCG 请求的拦截就由这个过滤器来实现。具体监控数据的生成和输出由一系列的 Slot 负责，具体原理这里不展开叙述，因为本文主要描述生产环境使用的改造，有兴趣的同学可以搜一下 Sentinel 时间窗口。原先的监控数据由 Dashboard 收集后存储在 Map 中，并且只存储五分钟内的数据，在 Dashboard 重启后监控数据就消失，这在生产环境中肯定是不能接受的。由于我们公司本身也有ES服务，这些数据又是个时序数据，自然而然就想到可以放入ES中进行持久化。首先新建 ElasticsearchMetricsRepository 来实现 MetricsRepository 接口类。分别实现其中的 save、saveAll、queryByAppAndResourceBetween 和 listResourcesOfApp。在 MetricController、 MetricFetcher 中把自动注入的 MetricsRepository 加上 @Qualifier(value = &quot;elasticsearchMetricsRepository&quot;)。还有另外一个点在于 Dashboard 查询监控数据进行展示时，开源的版本并不会将调用量为零的那段数据补齐，导致监控数据展示得非常奇怪。所以我们需要在查询监控数据接口对ES返回的数据进行补齐，让监控数据是连续的。下图是改造完的监控。 网关流控规则和 API 规则接入Apollo配置中心开源版本的 Sentinel 流控规则和API规则也都是内存态的，在 SCG 和 Dashboard 重启后也会消失。我们原先的配置中心采用的就是 Apollo，Sentinel 刚好可以跟 Apollo 进行集成。实现 Dashboard 推送至 Apollo，Apollo 再推送至 SCG，SCG启动时也从 Apollo 进行加载。 SCG方面跟普通应用接入 Apollo 一样，在 Apollo 建立应用。同时需要新增两个特殊的 namespace，分别为 flow_rule 、 api_group。在SCG中加入 Sentinel的 Apollo 依赖，同时配置一般的 Apollo 参数。然后需要对 Sentinel 的流控规则和API组规则加载方式进行修改。在 SentinelGatewayConfiguration 配置中加入如下两段代码 后续加载规则分别从某 namespace 下的某 key 加载。 Dashboard方面Dashboard 这边本身 Sentinel 已经把普通应用的流控规则从 Apollo 加载的代码已经写好放在 test 目录下，但是没有实现网关的代码。其实也很简单，Sentinel 的数据加载和推送的方式本身就提供了较好的扩展性。我们只需要分别实现 DynamicGatewayRuleProvider 和 DynamicGatewayRulePublisher。这里需要注意的一个点是 SCG 向 Dashboard 注册的 appName 最好和它本身在 Apollo 的 appId 保持一致。这样 DynamicGatewayRuleProvider在加载数据的时候直接用 appName 作为 appId 向 Apollo 请求配置。加载流控规则代码如下图所示。 至于 DynamicGatewayRulePublisher 其实有两个过程，第一步是向Apollo更新数据，第二步是要求 Apollo 对该 namespace 进行发布。代码如下图所示。 这里面涉及到的 Apollo 开放平台的授权之类的过程（这次也发现这是Apollo的一个非常强大的功能，实际上可以有很多的应用，特别是在自动化方面）请参考 Apollo 的文档。在这样的一个过程后很多同学应该能很自然而然地想到是不是可以在 Sentinel 或者其他监控工具监控到某些条件后自动地对 Apollo 的中的限流配置进行修改。是的，这也应该是我们以后的目标，做到限流的自动化和智能化。随后还需要新增 GatewayApiControllerV2，GatewayFlowRuleControllerV2，基本上就是把原有的GatewayApiController、GatewayFlowRuleController复制过来， 然后分别注入 ApolloProvider 和 ApolloPublisher。Controller 要改动的地方主要有两个，第一个将获取规则的方式从向SCG请求改为向 Apollo 请求，第二个是新增和修改规则先更新到 Apollo 再让Apollo 推送到 SCG。再将前端页面请求的路径指向新的 Controller。下图是获取API规则的代码改造前和改造后区别。ApiDefinitionProvider 就是 DynamicGatewayApiDefinitionProvider 的从 Apollo 获取 API 规则的实现。 此处还有个暗坑，一开始没注意到。本身Dashboard在新增规则时会生成一个ID，这个ID是个 AtomicLong, 随着Dashboard的重启自动就清空了，导致在重启后新增或修改规则会将ID=1的规则顶替。目前解决的办法是在新增和修改规则时加载目前的全部规则，然后在获取ID时获取当前全部规则的最大ID，从最大值开始递增。 登录和鉴权原先的登录和鉴权只提供了基础的登录功能且安全性很低。我们将其加入了公司的统一登录中心，实现了帐号密码及企业微信扫码登录和访问权限的控制。 尾声Sentinel 的改造陆陆续续进行了半个多月，其中也遇到了不少了问题，基本上都得到了较好的解决。目前仅仅是生产环境能用的状态，还有很多功能尚未完善。在改造的过程中也对Sentinel的基本原理和架构有了较清晰的认识，为后续使用中解决问题提供了基础。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[]},{"title":"针对流量的AB test 的一点思考","slug":"针对流量的AB test 的一点思考","date":"2020-04-08T05:19:40.257Z","updated":"2020-06-24T06:59:04.263Z","comments":true,"path":"2020/04/08/some-thoughts-on-ab-test-for-traffic.html","link":"","permalink":"http://xingcici.github.io/2020/04/08/some-thoughts-on-ab-test-for-traffic.html","excerpt":"","text":"之前公司提了个需求，不同比例的用户导向不同的落地页，统计转化效率。我调研了一些业界的方案后有了一些自己的思考。 在阅读下述方案之前 我们需要明确几个定义 AB TEST 方案分版本，不同的 AB TEST 方案拥有不同的标识，例如 ABTEST_2020_01_01。 某个确定的 AB TEST 版本下，会有不同的分组，每个分组也有确定的标识，例如 GROUP_A。下文中的 AB TEST 桶 ，指的就是不同的分组。 流量进入后会被打上一个或者多个 AB TEST 版本标识及 AB TEST 桶标识。 单纯针对该需求的方案目标: 相同用户返回相同落地页 不同用户根据比例返回不同落地页 跟踪不同落地页转化效率 方案: 根据ABTEST版本 建立唯一标识, 请求跳转链接时根据用的IP或者CookieId进行HASH，根据HASH命中区间( AB TEST 桶)返回不同的落地页，返回结果时带上TEST版本标识，转化效率的跟踪在H5进行。同时留有后门方便指定用户分入不同的区间中。 更通用的方案(也只是简略描述 具体还要根据详细的需求来细化)目标: 根据不同用户特征返回不同结果，跟踪不同返回结果的转化效率。 思路A 流量完全分层 所有流量可以循环利用 一个请求可以用于多个 AB TEST 方案: 设置要针对的特征，圈定范围用户。比如 用户的IP 用户的 OPENID 设置 AB TEST 方案，每个AB TEST方案都有唯一标识。再在 AB TEST 方案中，根据不同特征或者比例设置不同的 AB TEST 桶，每个 AB TEST 桶都有一个唯一标识用于跟踪，每个 AB TEST 桶可以设置一定比例的流量。 在网关设置过滤器，对进入的符合要求的流量特征进行HASH(同时也可以针对特定的用户标识强行进行分组,方便测试)，打上进行中的 AB TEST 方案的标识， 再根据该 AB TEST 方案打上 AB TEST 桶的标识。再对该流量进行下一次循环，查询是否还有 AB TEST 方案可以利用它，有的话则继续进行标记。符合最后的标记可能是类似于TESTA_GROUP1|TESTB_GROUP2。 流量到服务的时候，根据标识返回不同的结果，在返回的同时带上 AB TEST 方案的标识。 存在的问题：当不同的 AB TEST 方案对相同页面做实验时就会出现问题。 思路B 流量预先分桶 一个请求只能用于一个 AB TEST 方案: 设置要针对的特征，圈定范围用户。比如 用户的IP 用户的 OPENID。 设置实验方案，每个 AB TEST 方案都有每一标识。再在 AB TEST 方案中，根据不同特征或者比例设置不同的 AB TEST 桶，每个 AB TEST 桶都有一个唯一标识用于跟踪。 在网关设置过滤器，对圈定范围流量先随机进行HASH，按实验需要的流量大小随机分到不同的流量桶中，再在流量桶中实施具体的 AB TEST 方案， 再根据该 AB TEST 方案打上 AB TEST 桶的标识 例如 TESTA_GROUP1，随后直接进入服务端。 流量到服务的时候，根据标识返回不同的结果，在返回的同时带上 AB TEST 方案的标识。 存在的问题：当 AB TEST 方案逐渐增加时，后来增加的 AB TEST 方案可能无法获取到足够的流量 附图","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}],"tags":[]},{"title":"中小型公司数仓摸索和应用","slug":"中小型公司数仓摸索和应用","date":"2020-04-08T05:12:24.879Z","updated":"2020-06-24T06:59:04.261Z","comments":true,"path":"2020/04/08/exploration-and-application-of-data-warehouse-for-small-mediumsized-companies.html","link":"","permalink":"http://xingcici.github.io/2020/04/08/exploration-and-application-of-data-warehouse-for-small-mediumsized-companies.html","excerpt":"","text":"前言熬过创业初期的互联网公司一般发展迅速。一方面是业务的迅速发展，另一方面是数据的迅速累积。在获客成本越来越高的当下，谁利用好手中积累的数据来更好的服务于业务的发展，做到精细化运营，谁就能在竞争中获得领先优势。 对分析数据而言，数仓是必不可少的基础。在惠借科技之前的两年里，数据分析方面并没有一个清晰的数仓这样的概念，导致数据分析过程中一系列例如数据来源不一致，表过多和界限混乱等问题。最终导致的结果就是对业务响应慢，数据可能不准确。所有重新分析和搭建适合惠借的数仓是迫在眉睫的事情。惠借技术团队和BI团队经过两三个月，也算是初步完成了数仓的选型和分层。在此简单介绍下整个过程。 现状和需求分析目前公司内部BI团队是应用阿里云的maxcompute作为数据分析和平台。应用场景大部分是对用户部分行为进行分析和对订单进行分析，集群配置为10CU，数据量为几百GB，每天新增数据量最多几十GB。新的技术和平台肯定要能支撑起目前的业务需求同时能部分满足未来的扩展需求。 技术和平台选型正如前言所言，我们在选型时也面临着自建和使用第三方平台的问题。我们在一开始进行技术选型时也优先考虑自建Hadoop集群，并用Cloudera Manager在测试环境搭建了一套Hadoop环境。不得不说现在搭建一套Hadoop环境还是很方便的。Cloudera Manager集成了一系列的部署、配置和监控等功能。 然而并不是搭建一个Hadoop环境就万事大吉了。在Hadoop的调研和测试使用过程中，我们发现Hadoop对集群机器的配置要求很高，而且还有各种复杂的配置。其中配置又是特别重要的部分，需要针对不同的机器配置和实际情况进行配置才能达到最优的效果。现实情况是，我们目前并没有专业的偏向于这方面维护和调优的工程师，这就意味着自建的话在以后的使用过程中遇到各种问题的话，会有较高的维护和解决问题的成本。 我们转而将视线放在目前我们正在使用的 Maxcompute。看了Maxcompute文档后发现是阿里云基于Hadoop开发的云上大数据平台。我们目前服务器、线上数据库及日志都是采用阿里云提供的服务，而Maxcompute提供了一系列的与阿里云目前的产品配合的功能。例如数据直接从RDS导入，日志文件由LogHub投递到Maxcompute等。很大程度上方便我们的使用。同时减少了BI同学的学习成本。而后在经济成本方面进行了简单的计算发现，在我们目前的需要的配置下，直接使用Maxcompute的成本要低于自建Hadoop成本。 最后我们决定基于阿里云Maxcompte来搭建我们新的数仓体系。 数仓设计 上图就是我们目前的第一版数仓的设计。因为采用了Maxcompute，所以我们目前并不需要特别关注存储层的设计，而是专注于数仓层的设计。 数仓最底层是 近源数据层，该层的数据直接从各种数据源进入，仅仅进行一些简单的ETL过程。一般来说会直接和外部数据源的结构进行映射。 在近源数据层上方是 基础数据层，基础数据层的数据从近源数据抽取，经过清洗和转化后形成我们设计好的基础表等。 基础数据上方为宽表和主题表，该层的数据由基础数据层转化和组合而成。表的主题由BI同学对业务进行分析后得到。 最后数据形成 数据集市，这是能够直接满足业务需求的数据。这里的数据可以导出到RDS中提供给业务方使用。也可以提供给BI分析工具使用，例如我们计划使用的 SuperSet BI分析工具等。 规范其实在之前我们也是用 Maxcompute，那为什么还是需要重新构建呢。因为我们之前在使用的过程中，并没有参考数仓的一般规范及阿里云的Maxcompute最佳实践，导致使用过程中产生很多问题。最简单的例子就是表的命名没有遵循分层的命名规范，命名很混乱。所以定义 一系列的规范来约束开发过程是十分必要的,我们公司内部的规范仍在不断地完善中。 后话至此初步的数仓搭建已经结束，这是我们数仓的1.0版本，后续随着业务的变化和发展，数仓也会进行相应的调整来满足需求。","categories":[{"name":"数据智能","slug":"数据智能","permalink":"http://xingcici.github.io/categories/%E6%95%B0%E6%8D%AE%E6%99%BA%E8%83%BD/"}],"tags":[]}],"categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://xingcici.github.io/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"},{"name":"源码","slug":"源码","permalink":"http://xingcici.github.io/categories/%E6%BA%90%E7%A0%81/"},{"name":"中间件","slug":"中间件","permalink":"http://xingcici.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"编程","slug":"编程","permalink":"http://xingcici.github.io/categories/%E7%BC%96%E7%A8%8B/"},{"name":"架构设计","slug":"架构设计","permalink":"http://xingcici.github.io/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"},{"name":"分布式","slug":"分布式","permalink":"http://xingcici.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"日常问题","slug":"日常问题","permalink":"http://xingcici.github.io/categories/%E6%97%A5%E5%B8%B8%E9%97%AE%E9%A2%98/"},{"name":"数据智能","slug":"数据智能","permalink":"http://xingcici.github.io/categories/%E6%95%B0%E6%8D%AE%E6%99%BA%E8%83%BD/"}],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://xingcici.github.io/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"JDK","slug":"JDK","permalink":"http://xingcici.github.io/tags/JDK/"},{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://xingcici.github.io/tags/RocketMQ/"},{"name":"反射","slug":"反射","permalink":"http://xingcici.github.io/tags/%E5%8F%8D%E5%B0%84/"},{"name":"linux","slug":"linux","permalink":"http://xingcici.github.io/tags/linux/"},{"name":"网络","slug":"网络","permalink":"http://xingcici.github.io/tags/%E7%BD%91%E7%BB%9C/"},{"name":"Netty","slug":"Netty","permalink":"http://xingcici.github.io/tags/Netty/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://xingcici.github.io/tags/Zookeeper/"},{"name":"算法","slug":"算法","permalink":"http://xingcici.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"dubbo","slug":"dubbo","permalink":"http://xingcici.github.io/tags/dubbo/"}]}